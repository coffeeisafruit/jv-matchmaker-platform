Co-Sell Execution Platform Plan

Market Map & Competitor Teardown

Crossbeam: Founded in 2018, Crossbeam pioneered the partner ecosystem platform (PEP) category and today is the largest ecosystem-led growth network with over 30,000 companies on its platform ￼. Crossbeam connects partners to securely share CRM data and find overlapping accounts without exposing non-overlaps ￼. Key features include automated account mapping, fine-grained data sharing controls, and ecosystem analytics ￼. Crossbeam offers a free tier (Explorer, up to 3 seats) and paid plans (Connector at ~$150/user/month, or ** ~$4,800/month flat for unlimited partners** on higher tiers) ￼ ￼. This tiered model helped Crossbeam rapidly build a network effect: it’s widely adopted by mid-market SaaS firms (65% of its users are mid-market) ￼, yet also serves enterprises (16%) ￼. Crossbeam’s strengths are its massive partner network (18k+ companies as of last count) ￼, powerful data integration (CRM, data warehouses, CSVs) ￼, and robust governance (hashing data so only overlaps surface). It integrates with tools like Salesforce, HubSpot, Slack, and offers a Chrome extension (Copilot) for ecosystem insights ￼ ￼. Weaknesses: Crossbeam’s rich features come at a high price for full use, and it mostly surfaces “who to talk to” – the actionable follow-through still relies on manual effort or other tools ￼.

PartnerTap: Established in 2016, PartnerTap was the first mover in account mapping software, positioning itself as an enterprise-focused ecosystem sales platform ￼ ￼. Its focus is real-time, automated account and opportunity mapping for large enterprises with complex partner account teams ￼. PartnerTap touts 100% real-time data sync and deep CRM integrations, enabling partners to share and track pipeline and co-sell opportunities seamlessly ￼. It boasts impressive success stories (e.g. customers seeing 2900% increase in new deals in the first year of use) ￼. Unlike Crossbeam’s broad network approach, PartnerTap often works in 1:1 enterprise partnerships where both sides adopt it to manage shared accounts. Strengths include its enterprise-grade features (custom data handling, advanced security) and proven ROI (a reported ~3-month payback period, reflecting quick wins) ￼. Weaknesses: PartnerTap does not offer a free tier and keeps pricing private ￼ – adoption thus skews to large enterprises (57% of users are enterprise) ￼. Its network of companies is smaller than Crossbeam’s, meaning fewer partners are already on it (less of a built-in network effect). However, it excels in direct, deep partnerships where real-time collaboration is critical.

Reveal: Reveal (founded in 2020 in France) brands itself as a “collaborative growth platform” ￼. It emerged as the leading Crossbeam alternative, especially in Europe. Reveal has the second-largest ecosystem network (over ~12,000 companies) ￼ and is known for a highly user-friendly UX and strong emphasis on data privacy/GDPR compliance (owing to its Paris HQ) ￼. Like Crossbeam, Reveal offers a robust free tier to drive adoption ￼. It focuses on simplifying partner account mapping and relationship management – providing data-driven insights into overlaps and recommendations on which partners or accounts to prioritize ￼. Many users praise Reveal’s intuitive interface and guided workflows, which lower the learning curve for partner managers. Weaknesses: Reveal’s analytics and customization are a bit lighter-weight compared to Crossbeam (which caters to advanced use cases) ￼. Its presence in the U.S. market is growing but still trails Crossbeam’s; many North American companies first think of Crossbeam. That said, Reveal’s European stronghold and ease-of-use make it the choice for mid-market teams seeking quick value without heavy setup.

WorkSpan: Founded in 2015, WorkSpan takes a different approach as the #1 enterprise co-sell management network focusing on joint sales processes and cloud marketplace partnerships ￼. Rather than just finding overlaps, WorkSpan helps companies operationalize co-selling at scale – managing shared deals, referral tracking, and marketplace private offers within CRM workflows. It deeply embeds in Salesforce (via a managed package and APIs) so that alliance teams can “live” in CRM while co-selling ￼. WorkSpan is popular among large tech companies and cloud vendors – e.g. Palo Alto Networks, SAP, Databricks, MongoDB use WorkSpan for co-selling with hyperscalers ￼. Key features include automated deal registration/referrals, joint pipeline management, and metrics attribution for partner-influenced revenue. It also recently introduced AI assistants to automate account matching and task routing ￼. Strengths: WorkSpan is tailor-made for complex, multi-partner go-to-market motions (ISV-cloud co-sell, OEM partnerships, etc.), offering robust workflow automation and supporting multi-way data sharing. It emphasizes secure data sharing and governance, given the sensitive sales data involved in co-selling with big alliance partners. Weaknesses: WorkSpan is enterprise software (no free tier), requiring sales engagement and significant implementation (their pricing is custom, targeting large deals) ￼. Its use is concentrated in enterprises (57% of users enterprise-size) ￼, so mid-market companies find it too heavyweight. Also, WorkSpan’s network effect is limited – it’s often deployed bilaterally or within an alliance program, rather than an open network of thousands of partners.

PartnerStack: While PartnerStack is a partner relationship management (PRM) platform at its core, it has become a major player in the ecosystem space by enabling scalable partner programs. PartnerStack offers a partner portal for managing referrals, reseller deals, and payouts – it’s known for its large marketplace of 120,000+ active partners (agencies, affiliates, etc.) and claims to have driven over $2B in B2B partner sales through its network ￼. For co-sell teams, PartnerStack provides tools for pipeline sharing, deal registration, and even a “Discovery” network that can match you with potential partners ￼. Its key features include automated partner onboarding, a built-in partner CRM, campaign management, and payment automation for partner rewards ￼. Strengths: PartnerStack’s all-in-one PRM approach means it covers the full partner lifecycle (not just account mapping). It is especially strong for companies building partner programs from scratch – providing a large pool of ready-to-engage partners and out-of-the-box portal functionality. It’s highly popular with small and mid-size businesses (71% of its users are small businesses) ￼ due to a relatively accessible pricing model and ease of setup. Weaknesses: PartnerStack is less specialized in data overlap mapping; it integrates with Crossbeam for that capability ￼. Essentially, PartnerStack manages who your partners are and how you work with them, but it relies on tools like Crossbeam to identify which accounts to work on. Thus, PartnerStack alone isn’t a full co-sell execution solution; it’s more of a partner program infrastructure. Also, its focus on breadth (affiliates, referrals, etc.) means it may lack some advanced co-selling workflows that dedicated co-sell tools have.

Impartner (PRM): Impartner (est. 1997) is one of the most established PRM software providers. It offers a comprehensive channel management suite covering partner onboarding, training, deal registration, lead distribution, co-branded marketing, and performance analytics ￼. Impartner is known for its configurable partner portals and has a strong enterprise clientele. It’s often praised as a “fastest-growing, award-winning” channel management solution ￼, reflecting its significant market presence. Strengths: Rich feature set for end-to-end partner relationship management – companies use Impartner to run large channel programs with thousands of partners. It excels in workflow automation for partner deals (e.g. complex deal approval processes, incentive calculations) and integrates deeply with CRM systems. Weaknesses: As a legacy player, Impartner can be complex and expensive to implement (no self-serve). Its UI and user experience are sometimes cited as less modern than newer SaaS solutions. It doesn’t inherently provide account overlap discovery with other companies – it’s more about managing your direct partner interactions. As such, Impartner isn’t designed for ecosystem data sharing; it’s better suited to one-vendor-to-many-partners management (like a vendor managing resellers).

Zift Solutions (Unifyr) (PRM): Zift, recently rebranded to Unifyr ￼, is another leading PRM platform, notable for combining PRM with through-channel marketing automation (TCMA). Zift/Unifyr helps companies deliver marketing campaigns through their channel partners and manage leads and pipeline from those campaigns. It provides partner portals, content libraries, MDF (marketing funds) management, training, and analytics. Strengths: For organizations that heavily leverage partners in marketing and demand generation, Zift is a top choice – it integrates marketing automation with partner management. It aims at “bringing the channel together” profitably in one platform ￼. Zift is used by many large B2B tech firms and has a global footprint. Weaknesses: Like Impartner, Zift is a broad PRM and does not specialize in account mapping between companies. Its complexity can be overkill for smaller partner teams, and it typically requires custom configuration. Pricing is enterprise-level (custom quotes). Also, as indicated by its ROI average (~22 months payback) ￼, it’s a longer-term investment that needs scale to justify – not an easy quick-win tool for a small co-sell initiative.

Salesforce PRM: Salesforce offers PRM functionality as part of its Experience Cloud (partner portal templates and PRM add-ons). This essentially extends Salesforce CRM to external partners, allowing companies to share leads, opportunities, and content with partners in a secure portal ￼. Strengths: Native integration with Salesforce CRM – data flows seamlessly, and partners can be managed as just another part of your CRM (with appropriate sharing rules). If an organization already uses Salesforce, the PRM can be a natural extension to give partners access to certain records (leads, opportunities) and to track partner-influenced deals. It supports deal registration, partner scoring, and the basics of collaboration. Weaknesses: Salesforce PRM is not a standalone product – it requires a Salesforce org and is geared towards companies deeply invested in that ecosystem. Feature-wise, it often lags dedicated PRM vendors in user experience and advanced functions (e.g. robust training modules or marketplace features). It also does not inherently do multi-company account matching; any Crossbeam-like functionality would have to be achieved by manually comparing CRM data or using an app. Many companies using SF PRM still use Crossbeam/Reveal on top for ecosystem mapping. Additionally, cost can grow quickly (partners need portal licenses, etc.), and ROI tends to be longer (20+ months payback on average) ￼, indicating a significant upfront investment in setup and adoption.

Summary: The partner tech landscape is crowded but can be categorized into (a) ecosystem data & account mapping tools (Crossbeam, Reveal, PartnerTap, Superglue, etc.), (b) co-sell orchestration and alliance management (WorkSpan, also PartnerStack to an extent), and (c) traditional PRMs (Impartner, Zift/Unifyr, Salesforce PRM, etc.) ￼. Crossbeam and Reveal lead in network size and are the go-to for discovering overlap opportunities, especially in mid-market companies ￼ ￼. PartnerTap and WorkSpan cater to large enterprises with deeper integration into sales processes. PRMs like Impartner and Zift manage the partner funnel and program at scale, but need to be paired with mapping tools for ecosystem-led growth. This competitive analysis reveals an opportunity for a new entrant to differentiate either by better execution (automation) of co-selling or by targeting an underserved segment or vertical niche, as discussed next.

Wedge Hypotheses & Evaluation

To break into this ecosystem-led growth space, we considered three potential market entry wedges:
	1.	“Execution OS” – Signal→Action Automation: Position the product as the operating system for co-sell execution. The focus here is on automating the workflow from partner signal to sales action. Today, mapping tools identify which accounts are overlapping, but partnership teams still struggle with what to do next and must manually coordinate follow-ups ￼. This wedge would build an automation engine that listens for “partner signals” (e.g. a new overlap, a partner deal progressing, a partner rep requesting an intro) and triggers “actions” (create tasks, send Slack alerts, log CRM activities) without human delay. In effect, it transforms static overlap data into a dynamic Execution OS that ensures every identified opportunity is acted upon through rules and playbooks. The target users would be companies already doing account mapping (perhaps with Crossbeam/Reveal exports) who need to scale up the execution and collaboration part – likely more mature partner teams or any org that has experienced the “lot of manual work after mapping” pain point ￼. Value prop: faster partner deal cycles, no opportunities falling through cracks, and quantifiable partner-sourced revenue growth through automation. Key differentiators: A built-in rules engine and workflow library (e.g. auto-notify the account executive via Slack and create a CRM task when a new mutual opportunity arises), and possibly an AI component to prioritize partner signals. This wedge leverages the white space that even Crossbeam acknowledges: “the work after account mapping” needs streamlining ￼. Risks: It may be ahead of the market for less mature programs – if a company hasn’t even done basic mapping, they might not seek automation yet. Also, it likely requires integrations into many systems (CRM, Slack, etc.) from the start, which raises development complexity.
	2.	Mid-Market Self-Serve – Mapping + Co-Sell for the Masses: Target the underserved mid-market and SMB segment with a self-service account mapping and co-selling platform. This wedge aims to be “Crossbeam Lite + execution” for companies that find Crossbeam’s enterprise pricing or complexity a barrier. The mid-market (companies with say 50-500 employees) increasingly want to co-sell, especially those using HubSpot CRM (popular for mid-market) ￼. Our platform would offer a frictionless sign-up, easy integration to HubSpot (and Salesforce for those who have it), and out-of-the-box basic co-sell workflows. Think of a product-led growth approach: anyone can join for free, connect their data, and start mapping with partners in minutes – no heavy sales process. We’d emphasize intuitive UI and guidance (perhaps leveraging Reveal’s UX lessons) and a “freemium” model to drive viral growth (because the value increases as more partners join). The included co-sell features (beyond overlap identification) might be lightweight: e.g. the ability to send a partner an introduction request for an overlapping account, or a shared notes/timeline for each overlapping deal. Value prop: bring ecosystem-led growth to companies that previously stuck to spreadsheets or could only partially use free Crossbeam due to seat limits. The TAM here is large – tens of thousands of SaaS companies and channel-centric businesses globally. We lower the entry bar: HubSpot users, for instance, can align with partners “without the overhead of enterprise-grade systems” ￼. Risks: The mid-market focus pits us directly against Crossbeam’s free tier and Reveal’s free offering – a competitive crowd. The network effect advantage of incumbents is strong; partners might say “we’re already on Crossbeam.” Also, smaller customers may churn if they don’t quickly see value (so onboarding and education is critical). Monetization could be tricky if too many stay on free; we’d need compelling premium features or usage-based limits.
	3.	Vertical Ecosystem Graph + Workflows: Carve out a specific industry vertical or ecosystem niche and build a tailored solution for it. The idea is to create a “vertical ecosystem graph” – a knowledge graph of companies, customers, and partner relationships specific to that sector – combined with custom workflows that industry players need. For example, focus on the cloud infrastructure ecosystem (where co-selling with AWS/Azure and cloud marketplace listings are key), or the cybersecurity ecosystem, or perhaps the manufacturing tech channel. By focusing narrowly, we can incorporate vertical-specific data and integrations: e.g. for cloud, integrate with AWS ACE for deal registration, or for a healthcare vertical, include compliance workflows for sharing data. The platform’s mapping would not just be raw account overlaps, but enriched with industry data (perhaps pulling from third-party databases or public sources to identify likely overlaps or complementary offerings). Value prop: A purpose-built tool that speaks the language of the industry’s partnerships – providing not just generic account overlaps but insights like “this customer is using X and Y; a partner offering Z could complete the solution”. It could also provide pre-populated ecosystem data for new users (e.g. a vertical market map). This wedge can gain initial traction by solving a pain-point unique to the vertical: for instance, in the cloud co-sell space, alliance managers struggle to manage 3-way co-sell deals (vendor + cloud + reseller) – we could be the first to elegantly support that workflow. Risks: A vertical niche inherently limits initial market size – if the vertical is too narrow, revenue growth could stall or require expanding to additional verticals later (which is like launching new products). It also demands deep domain expertise on the founding team or via advisors. Incumbents could still adapt to compete if they see traction in that vertical. We’d need to ensure that the first vertical we pick has enough unmet need and propensity to adopt new tech.

Wedge Evaluation Criteria: To systematically evaluate these wedges, we scored them on a few dimensions (1-10 scale): Market Size & Demand, Competitive Differentiation, Feasibility/Complexity, Monetization Potential, and Strategic Upside (long-term expansion).
	•	Market Size/Demand: Execution OS targets a subset of existing partnership-enabled companies (moderate sized market, as only ~30% of partner programs might be ready for automation). Mid-market self-serve addresses a broad market (thousands of companies globally) with growing ecosystem interest, hence high score. Vertical niche might be smaller initially (depending on vertical, e.g. “cloud ISV co-sell” has maybe a few hundred prime targets, albeit high value).
	•	Differentiation: Execution OS is highly differentiated – none of the major players market themselves as automation-first (this could create a new subcategory). Mid-market self-serve differentiation is medium – essentially a positioning against incumbents on price/usability rather than a fundamentally new capability. Vertical graph is highly differentiated if we truly tailor to an industry; we’d likely be the only game in town for that niche initially.
	•	Feasibility (Tech & Go-to-Market): Execution OS has higher technical complexity (needs building robust rule engine and deep integrations early) which could slow MVP, and a harder GTM (we’d sell to advanced teams, perhaps needing more consultative sale). Mid-market has lower complexity (start with basic mapping and a few integrations) and a PLG go-to-market – more feasible to launch quickly and grow virally, though success depends on network effects. Vertical approach complexity varies – e.g. building a cloud co-sell graph might require ingesting large datasets or custom integration with AWS/etc., so moderately complex; GTM is easier in that you can tightly target industry forums and use case, but you risk being pigeonholed.
	•	Monetization & Unit Economics: Execution OS likely means higher ACV per customer (enterprises will pay for automation that directly ties to revenue outcomes) – so fewer customers needed, but longer sales cycles. Mid-market means lower ACVs (maybe a $5k-$20k annual contract for a mid-size firm), but the volume can compensate; however, SMB churn can be an issue. Vertical – if the vertical’s players have money (e.g. cloud ISVs, cybersecurity firms), you could charge a premium because it’s specialized (potentially a high willingness to pay for tailored value), but the count of customers is limited.
	•	Strategic Upside: Execution OS, if successful, could become the central operating system for all ecosystem activities, a very defensible and expandable position (score high). Mid-market success could allow going upmarket later, but you risk being seen as “the SMB tool” and facing churn if those companies graduate to bigger solutions; still, the network you build could be valuable (score medium). Vertical – can expand horizontally to other verticals or broaden to general after dominating one, but that transition can be tricky (score medium).

Scoring Summary: (out of 10 for each criterion)
	•	Execution OS: Market 6; Differentiation 9; Feasibility 6; Monetization 8; Strategic Upside 9. Total ~38/50.
	•	Mid-Market Self-Serve: Market 9; Differentiation 6; Feasibility 8; Monetization 6; Strategic Upside 7. Total ~36/50.
	•	Vertical Focus: Market 5; Differentiation 8; Feasibility 7; Monetization 7; Strategic Upside 6. Total ~33/50.

(These scores are approximate, for internal comparison.)

Selected Wedge: Execution OS (Signal→Action Automation) is our chosen wedge. Despite a slightly smaller initial market, its high differentiation and strategic upside make it the most compelling. It directly addresses a pain felt even by users of existing solutions – the need to actually execute on partnership data – which gives us a clear value prop that others lack. We can still serve mid-market customers (especially tech-savvy ones) with this wedge, but we’ll lead with the narrative of “turn your partner overlaps into real pipeline via automation.” This focus also aligns with our team’s strength in building workflow automation, and we believe it’s a story that can attract early adopter customers who are vocal in the ecosystem community (driving word-of-mouth). In short, Execution OS is our wedge to enter the market: we’ll still deliver core mapping as a feature, but our product and messaging will revolve around action and automation as the differentiator.

Product PRD: MVP to V2

Personas & Jobs-to-be-Done

We identify two primary user personas and one secondary persona for the platform:
	•	Partner Manager (Primary Persona): This is a Head of Partnerships or Partner Account Manager responsible for driving co-selling with partner companies. Jobs-to-be-done: Connect with new partners and securely share relevant data; discover which accounts or opportunities to collaborate on; initiate co-sell plays (e.g. ask partner for an intro to a customer, share a lead); track the progress and outcomes of co-selling efforts; report on partner-sourced pipeline and revenue. Pain points: Partner managers currently juggle spreadsheets or multiple tools to map accounts, communicate via email/Slack externally, and manually update CRM – they need a single pane of glass and automation to keep things moving ￼.
	•	Account Executive / Sales Rep (Primary Persona): This is an AE or sales rep whose accounts could be influenced by partners. They usually come into play once a partner overlap is identified on one of their target accounts. Jobs-to-be-done: Get notified when a partner can help open a door or accelerate a deal; easily collaborate with the partner rep (e.g. schedule a joint call, share context); and update the pipeline status accordingly. Pain points: Sales reps often ignore partnership data if it’s not served in their workflow. They need timely, actionable alerts (in tools they use daily like CRM or Slack) about partner insights on their accounts, and they want the introduction process to be quick and low-effort.
	•	Partnership Ops / CRM Admin (Secondary Persona): A technical/ops role that sets up integrations and ensures data flows correctly (e.g. the Salesforce admin or RevOps person supporting partnerships). Jobs-to-be-done: Connect the platform to internal systems (CRM, data warehouse), map data fields, configure access controls, and monitor data quality. They care about security, minimal maintenance, and that the tool doesn’t break their CRM. This persona will be critical during onboarding and scaling, though they won’t be day-to-day users beyond monitoring.

Using the above personas, we define a few core use cases (JTBD) that our product must support:
	•	JTBD 1: “Find Overlap Opportunities.” As a Partner Manager, I want to securely compare my accounts with a partner’s accounts to identify mutual customers, open opportunities, or target prospects we have in common, so that we can focus our co-selling efforts on the most promising overlaps. Acceptance Criteria: Given two connected partners, the system computes a list of overlapping accounts (by defined matching rules) and displays key info (account name, owner, relevant status from each side). Both parties should only see the overlap, not each other’s full lists ￼. The overlaps can be filtered (e.g. show overlaps where my status is “Open Opportunity” and partner’s status is “Customer” – a ripe combo for an intro).
	•	JTBD 2: “Take Co-Sell Action Effortlessly.” As a Partner Manager or AE, once an overlap is identified, I want to seamlessly take the next step – for example, request an introduction to a decision-maker from my partner, or share a mutually relevant insight – all within one platform, so that the opportunity moves forward. Acceptance Criteria: On an overlap account entry, user can click “Request Intro” which sends a templated message to the partner’s relevant rep (in-app and via email notification). The partner rep can approve/decline the request. If approved, the platform facilitates sharing the contact details or scheduling a meeting. All these actions are logged (with timestamps and statuses) in the context of that account.
	•	JTBD 3: “Automate Follow-ups and Notifications.” As a Partner Manager, I want routine tasks (alerting sales reps, updating CRM fields, reminding partners about pending requests) to happen automatically based on defined triggers, so that nothing falls through cracks and everyone stays informed without me micromanaging. Acceptance Criteria: Examples: When a new overlap is found that meets certain criteria (e.g. high deal value), the account executive gets a Slack DM with details ￼. If a partner hasn’t responded to an intro request for 7 days, the system sends a reminder or notifies the PM. When a partner marks a referred opportunity as “Closed Won” on their side, our CRM is automatically updated (via integration) with a win attribution note. Users can configure these rules via a UI (no-code if possible).
	•	JTBD 4: “Measure Partner Impact.” As a Partner Manager or an Executive, I want to track KPIs like how many overlaps we have with each partner, how many introductions have been made, how many deals sourced or influenced by partners, and the resulting revenue – so I can prove ROI and optimize the program. Acceptance Criteria: The product provides dashboards (or reports) showing metrics per partner and in aggregate: e.g. number of overlaps, intro requests (pending, accepted, conversion rate), pipeline $ from partner-influenced deals, and actual revenue closed from co-sell deals. These should be filterable by timeframe and exportable for exec reporting.

These JTBDs drive the scope of our product versions.

Minimum Viable Product (MVP)

The MVP will focus on delivering the core account mapping and basic co-sell action loop for 1-1 partner collaborations, with an initial emphasis on automation within that limited scope. We prioritize a working end-to-end flow: connect data → see overlaps → take an action → track outcome. Below are the MVP features with acceptance criteria and design details, including how they map to data, UI, integration, and KPIs:
	•	Organization & User Setup: Feature: Onboarding flow for a new company. DB: Organization table (each company/tenant), User table (with role, e.g. Org Admin vs Member). UI: Signup page, Org settings screen. Integration: Integration settings (but actual CRM connect is separate feature). Acceptance Criteria: A user can sign up (email/password or SSO), create their Organization, invite colleagues (via email invite). They can define basic profile info for the org (name, industry, etc.). Org Admin can manage users/permissions (e.g. designate who can invite partners). KPI Impact: Tracks number of org sign-ups, active users invited. A successful onboarding correlates with a higher conversion to active usage (KPI: onboarding completion rate).
	•	CRM Data Import & Account Store: Feature: Connect to CRM and import accounts. DB: Account table (stores accounts/leads from CRM, linked to Org), potentially AccountHash for hashed identifiers. UI: “Connect CRM” wizard and a preview of imported records count. Integration: Salesforce and HubSpot API integration (OAuth flows) for pulling account data ￼. Acceptance Criteria: An Org Admin can connect at least one data source (e.g. Salesforce: it authenticates and allows selecting objects like Accounts/Opportunities; or HubSpot companies) ￼. The system imports key fields (e.g. account name, domain, owner, stage, ARR) and continues to sync periodically. Data is stored securely per org. After import, the user sees a count of total accounts and can browse their own account list in-app to verify data. KPI Impact: Data completeness – e.g. % of customers that have connected a CRM within first week (higher means quicker time-to-value). Also number of accounts imported (drives likelihood of finding overlaps).
	•	Partner Onboarding & Partnership Management: Feature: Invite a partner organization and establish data sharing rules. DB: Partnership table (represents a mutual connection between two Orgs, with status pending/active), SharePolicy table (rules for what data each side shares). UI: “Invite Partner” form (enter partner admin’s email, message) and a pending invitation dashboard; partnership detail screen to adjust sharing settings. Integration: Email integration to send invites; could later integrate with an existing Crossbeam network (if we allow discovery of known companies, but MVP keep to direct invites). Acceptance Criteria: User A can invite Partner B via email. Partner B’s admin clicks the link, signs up their Org, and the partnership is established once both sides agree. Each side can configure which segments to share (e.g. share only accounts in region X or only open opportunities). The system should have default “share all accounts” for simplicity MVP, but allow toggling of at least basic filters or a safe mode (like “only share account names, not full details, until I approve”). Once active, both partner orgs are now able to map data. KPI Impact: Partnership activation rate – how many invited partners actually join (this is critical for network growth). Also tracks average number of partners per org (we expect maybe 1-2 in early use, growing as the network effect kicks in).
	•	Account Overlap Matching: Feature: Compute and display overlapping accounts between two partner orgs. DB: Overlap table (or a join result view) containing pairs of Account references (Org A’s account and Org B’s account) that match. UI: Overlaps dashboard – user selects a partner to view overlaps with; the UI lists matching accounts with columns from both sides (e.g. my status vs partner status). Integration: None external (uses internal data from both orgs); potentially use an external service for fuzzy matching (not MVP, deterministic only for now). Matching logic: MVP will use deterministic matching on a stable key: e.g. account domain or exact name match after normalization. We will likely use hashed email domain or website URL as the join key for privacy (both sides hash their domains, and the system finds identical hashes) – this ensures neither side sees non-matching account info ￼. Acceptance Criteria: After both partners have data loaded, the system finds overlaps automatically. For MVP, we can start with simple rule: match on exact website domain (if available for both accounts), falling back to exact company name match if no domain. The overlaps list is populated and updated in near-real-time (after any new data sync). Both partners see the same list (each sees their own account name and partner’s account name, which should represent the same entity). If there are too many overlaps, allow search/filter by name. Must ensure that no non-overlap data leaks – e.g. if one side has an account that doesn’t match, the other side cannot infer anything about it (it remains invisible). KPI Impact: The core value metric: number of overlaps found per partnership. We expect at least a handful of overlaps for most connected partnerships – if this number is zero or very low, the perceived value drops. So we’ll monitor what % of partnerships yield overlaps and how many; that will also drive usage of next features.
	•	Overlap Detail & Collaboration (Basic Co-Sell Flow): Feature: For each overlapping account, enable basic collaboration actions – specifically, an Introduction Request workflow. DB: Opportunity or Collaboration table – when an intro request is made, record it, along with status (requested/accepted/declined) and references to the account and partner users. UI: On the overlaps table, each row has an “Request Intro” button (or similar “Take Action” menu). Clicking opens a modal with a pre-filled message template (editable) to send to partner. Partners have a corresponding interface to view incoming requests (an “Inbox” of partner requests) and respond. Also, an overlap detail page shows a timeline of activities (requests, messages). Integration: Email or Slack notifications to the partner about the request. If the partner accepts, could integrate with calendar or contact sharing (MVP, a simple text exchange of contact info is fine). Acceptance Criteria: A Partner Manager or AE on side A can send a structured intro request to side B for a specific overlap account. Side B’s designated user receives a notification (in-app, plus email). Side B can approve and optionally provide the contact or introduction details (like “Sure, I’ll introduce you to Jane Doe at Acme Corp next week” possibly filling a small form). Upon approval, side A is notified and the overlap record is marked as “Intro Accepted.” If declined, a reason can be given. Both parties can add notes/comments on the overlap record for context (e.g. “Partner B says they’re working an upsell, will loop us in after that closes.”). KPI Impact: Engagement metrics like number of intro requests sent and % accepted. This directly correlates to pipeline generated. A high ratio of overlaps→requests and requests→acceptances is a sign of delivering value. These actions will also feed into the ROI metrics later (e.g. how many partner meetings set up).
	•	Notifications & Basic Automation: Feature: Real-time alerts for key events to drive engagement. DB: Could log events in an Event table; user notification preferences in a UserSetting table. UI: Notification bell icon in app with a feed of recent events (requests, partner actions). Basic settings page to subscribe to certain notifications (e.g. toggle email or Slack alerts). Integration: Slack App integration for notifications, and email service. MVP automation rules (hardcoded initially) like: “Notify account owner via Slack DM when partner accepts an intro and shares contact info.” We will include a simple Slack integration such that our app can post messages to a company’s Slack (via a webhook URL or Slack app install) ￼. Acceptance Criteria: Events in the system (like “New overlap found”, “Partner X joined”, “Intro request received/accepted”) trigger notifications. By default, send an email to relevant users (e.g. the person who sent a request gets an email on response; partner manager gets email when a new partner connects). If Slack is connected (optional setup), the notification also appears in Slack – e.g. we can allow the user to link a Slack channel or Slack Connect channel with the partner for co-selling ￼. MVP might limit to internal Slack (notify your team’s channel or AE). KPI Impact: User engagement frequency – e.g. daily active users or weekly active. Good notifications drive people back into the app and keep momentum. Also speed of response can be a KPI (how fast are intro requests answered – hoping notifications shorten this).
	•	Partner Dashboard & Basic Reporting: Feature: A simple dashboard providing at least counts and statuses to the partner manager. DB: Could aggregate from overlaps and requests (or use SQL queries on those tables). UI: A “Home” dashboard showing: number of partners connected, total overlaps, overlaps by stage (e.g. X intros requested, Y accepted), and maybe a list of recent activities. For MVP this can be minimal and non-customizable. Integration: None external, just internal data. Possibly allow export of a CSV of overlaps or results for offline analysis. Acceptance Criteria: The partner manager can log in and immediately see an overview: “You have 3 partners connected. 120 overlapping accounts identified. 5 intro requests sent this month (3 accepted, 2 pending). Partner-influenced pipeline: $200k (estimated).” The pipeline number might require manual input or integration with CRM (which could be a stretch for MVP – could skip real dollar calculation until V1). At minimum, allow them to mark an overlap as “in pipeline” or “won” for tracking manually. KPI Impact: This doesn’t directly have an external KPI, but it’s important for perceived value – we might measure dashboard usage or report exports. Ultimately, these figures feed exec-level KPIs (like partner-sourced pipeline), which in pilot phase we’ll measure manually.

Each feature above touches specific database tables, UI screens, integrations, and contributes to metrics that demonstrate value (we summarized those in each). The MVP acceptance criteria overall is that a pair of partner companies can go from zero to having identified overlaps and exchanged an introduction via the platform, with minimal friction and secure data handling throughout. If our pilot users can achieve a closed co-sell deal or at least a few meetings thanks to the product within the first 1-2 months, that validates the MVP.

Version 1.0 Enhancements (Post-MVP)

Once the core loop is proven, V1 will deepen the functionality, robustness, and start to add scalability. Planned V1 features include:
	•	Advanced Matching & Identity Resolution: Extend beyond exact matches. Introduce fuzzy matching (e.g. if “Acme Corp” and “Acme Corporation, Inc.” are likely same) and maybe a deterministic + probabilistic hybrid. DB: perhaps an AccountAlias or MatchCandidate table storing potential matches with scores. UI: An interface for partner managers to review & approve fuzzy matches (“These 5 look like the same account, confirm?”). Integration: Possibly use a third-party service or library for similarity scoring (or integrate with a dataset like Clearbit for company normalization). Acceptance Criteria: The system suggests overlaps that are not 100% exact but likely (with a confidence score). Users can approve suggestions to turn them into confirmed overlaps. Deterministic (exact domain) matches still auto-confirm. The model might use rules like: same normalized name & country -> high confidence, or matching parent company, etc. This increases overlap count by catching cases like spelling differences. KPI: Overlap coverage – measure increase in overlaps found due to fuzzy logic (target a significant lift, e.g. +10-20% more overlaps identified). Also track false positive rate through user feedback (should be low; user-approved mechanism ensures precision).
	•	Partner Pipeline & Opportunity Tracking: Instead of treating overlaps as static, allow tracking the status of co-sell opportunities. DB: Possibly an Opportunity table that links to Overlap (one overlap could spawn an Opportunity if both decide to pursue a deal). Fields for stage, value, owner, etc. UI: A “Co-Sell Pipeline” view that looks akin to a sales pipeline: e.g. columns for “Intro Requested”, “Intro Accepted”, “In Discussion”, “Closed Won/Lost”. Both partners can update progress (or it syncs from CRM if integrated). Integration: Sync with CRM objects – e.g., if an intro leads to a new Opportunity in Salesforce, we could capture that via Salesforce API (need read/write integration to CRM for opportunities). Also integrate with partner’s CRM similarly, or allow manual update. Acceptance Criteria: Users can convert an overlap into a “co-sell deal record” and track its lifecycle. Milestones like “meeting held” or “proposal sent” can be noted. If CRM is connected, we attempt to link this to an actual CRM opportunity (maybe by matching account + contact or allowing the user to select their CRM opp to associate). Both sides should see a shared view of progress (with appropriate confidentiality if needed). KPI: Partner-influenced pipeline ($) becomes measurable within the app. We’d want to see a growing number of “co-sell opportunities” created and their progression rates. This ties our value to revenue outcomes, which aids expansion and retention.
	•	Automation Rules Builder (No-Code): Expand the basic notifications into a configurable rules engine for signal→action. DB: AutomationRule table storing triggers and actions; possibly a PendingAction queue table. UI: A settings interface where users can create rules like “When [event/condition], do [action]”. For instance: Trigger: “New overlap with Partner X where our Stage = ‘Open Opportunity’ and their Stage = ‘Customer’” → Action: “Create task in Salesforce for account owner” or “Send Slack alert to #partner-sales channel”. Integration: Many actions will be integrations: create CRM tasks via API, send Slack message (we’ll extend Slack app permissions to post to certain channels or as bot DM), send email, or even call a webhook for custom actions. Acceptance Criteria: Users (likely Org admins or partner managers) can create at least a few template rules. Provide pre-built suggestions (e.g. a toggle: “Auto-notify reps of new overlaps” which under the hood creates a rule for Slack/email). The rule processing should be reliable (perhaps processed by background worker). Logging for each rule execution should be available (so ops can debug if something misfired). KPI: Automation adoption – e.g. % of orgs that set up at least one custom rule. Ultimately we believe organizations with automation will see faster partner deal cycles, so we’d also measure time from overlap to intro or to opportunity creation as a metric and aim for improvement where automation is used.
	•	Salesforce & HubSpot Data Push (Integration Enhancements): In MVP we pull data; in V1 we will push insights back to CRM. For example, create a custom object or field in CRM for partner overlaps ￼. Crossbeam does this with a “partner overlap object” in Salesforce ￼; we will do similarly. DB: We maintain mapping of our records to CRM record IDs (e.g. an Account.crm_id field). UI: In the integration settings, user can enable “write-back to CRM” and map which fields to update (or allow default: e.g. add a checkbox “Has Partner Overlap” on Account, and maybe a related list of overlapping accounts via custom object). Integration: Use Salesforce API (or HubSpot API) to upsert records. Also possibly install a small managed package or provide a component for Salesforce to display partner data inside Salesforce UI. Acceptance Criteria: After enabling, a sales rep looking at an account in Salesforce can see if there’s a partner overlap (via a flag or related list showing partner org name and status). This reduces context-switching. Also, if an overlap moves stages (e.g. intro done), we could update a field “Partner Intro Status” on the account or opportunity in CRM. For HubSpot, similarly perhaps populate a custom property or timeline event. KPI: Integration depth – measure usage of CRM integrated features (are sales reps viewing partner data in CRM? We might track number of CRM records updated or how often the SF component is loaded if possible). The broader KPI is sales adoption: more reps engaging because they see partner info in their system of choice.
	•	Expanded Integrations: Add at least one or two additional integrations beyond core CRM/Slack. For example, a Google Chrome Extension (CoSell Copilot) similar to Crossbeam’s ￼. Feature: Chrome extension that detects the current website or CRM page a user is on and surfaces partner data. Use case: When a sales rep is on a LinkedIn profile or a company’s website, the extension icon can show “This company is in your partner network: Partner X has an open deal here – click to view details”. Or while in Salesforce web UI on an account record, it could highlight partner overlaps (if user doesn’t have the native integration). DB/Integration: The extension queries our backend via API for data on the domain or account name. We’ll need an API endpoint for “get overlaps by domain/name” with appropriate auth. Acceptance Criteria: The Chrome extension is published and a user with our app can install it, and when on e.g. “acme.com” or a LinkedIn page for Acme Corp, it shows a small popup: “Acme Corp overlaps with PartnerCo (they are a customer of PartnerCo). Click to request an intro.” This essentially brings the platform’s insights to wherever the user is browsing ￼ ￼. Another integration could be a Slack command to lookup overlaps (Crossbeam’s Slack app allows /crossbeam company-name to search overlaps in Slack) ￼. We can implement that too. KPI: Reach of data – e.g. number of extension uses or Slack command uses. The goal is to increase accessibility and reduce friction, which should reflect in overall engagement and perhaps shorter response times.
	•	Security & Access Controls Enhancements: By V1, as more data flows, we need robust controls. Introduce role-based access in the app (e.g. Partner Manager vs Sales Rep roles, with permissions like who can invite partners, who can see what data). Feature: Possibly allow restricting certain overlaps to certain users (if needed). Also implement audit logs for data access to assure security teams. Acceptance Criteria: The system should meet common security reviews: clearly delineated data silos per tenant, encryption at rest and in transit (already planned), and perhaps a feature to allow an org to approve what fields are shared (so they can exclude highly sensitive fields). KPI here is simply security compliance – passing customer security assessments (not a user metric, but crucial for enterprise sales by V2).

V1 summary: It focuses on making the platform truly enterprise-ready and more automated. We’ll go from a functional MVP to a more polished product with key competitive features: better matching (on par or better than Crossbeam’s algorithms), deeper CRM integration (like Crossbeam’s custom objects and Slack integration, which we match ￼ ￼), and the signature differentiation: a user-friendly automation builder embedded in the product (something competitors lack). V1 will also incorporate feedback from pilot users about edge cases and UI tweaks.

Version 2.0 Vision

V2 (beyond the initial 6 months, roughly) will further expand capabilities, especially around multi-partner collaboration and advanced analytics/AI. Some planned V2 features:
	•	Multi-Partner Ecosystem Mapping: Extend from bilateral (one-to-one) to multi-partner overlaps. For example, identify if three companies all share a customer in common (useful for forming alliance plays). UI: a network graph view or ability to select multiple partners and find common overlaps (the “Venn diagram” of overlaps). DB: More complex queries on the Account data across multiple orgs – perhaps using an intermediary hashed key store to quickly find intersections. Acceptance Criteria: A partner manager could see, for instance, “Account X is common to you, Partner A, and Partner B” – enabling a three-way co-sell opportunity (e.g. a solution integration sale). This is advanced but truly leverages network effect once there’s a sizable data set.
	•	AI-driven Partner Insights: Leverage the accumulated cross-org data to provide recommendations: e.g. “You have 50 accounts with overlap, but these 5 are high propensity: Partner’s product complements yours and the timing is right based on intent data.” Or “Suggest new partners” by analyzing gaps (perhaps like a suggestion engine: “Companies similar to you often partner with X”). This could use machine learning on the graph of partnerships and outcomes. KPI: this would aim to increase the success rate of overlaps turning into deals (by focusing effort).
	•	Workflow Marketplace: Provide pre-built “playbooks” or automation recipes (like a library of common rule templates, or even multi-step workflows) – e.g. a “30-60-90 day partner engagement plan” that automates touches with new partners (Superglue’s approach hints at this ￼). Possibly allow community sharing of workflows if privacy allows.
	•	Additional Integrations: By V2 we’d integrate deeper with communication tools (e.g. Microsoft Teams in addition to Slack for companies that use it), other CRMs (Dynamics 365), and perhaps marketing automation (to trigger partner marketing campaigns for overlaps). A CRM-side widget (like a Salesforce Lightning component or HubSpot card) to fully operate overlaps from within CRM could also be delivered, improving the in-context experience for sales. We may also integrate a partner’s PRM if applicable – e.g. if they use PartnerStack, we might push referral leads into that system ￼.
	•	Scalability & Performance upgrades: V2 will include behind-the-scenes work: sharding or partitioning the database if needed as we scale to hundreds of orgs and millions of accounts (discussed in Tech Arch below), and possibly introducing search indexes for faster fuzzy matching and retrieval.
	•	Monetization features: If we adopt a usage-based pricing (say charging by number of accounts or partners connected beyond a threshold), V2 might include admin dashboards for usage quotas, billing integration, etc. Also role-based tier features (some features only for higher tiers, which might require gating in UI).

Throughout MVP, V1, V2, we will maintain a jobs-to-be-done focus: ensuring each new feature clearly helps the user get their job done more efficiently. We’ll also document edge cases and incorporate them:

Key Edge Cases & How We’ll Handle Them
	•	Data Mismatch/Duplicates: What if one company’s CRM has duplicate accounts or different naming? Our identity resolution module (fuzzy matching in V1) will attempt to handle common duplicates. Edge case: If one org has two records “ACME Inc.” and “Acme Corporation” that are actually same account, it might appear as separate overlaps with partner’s single “Acme Corp” record. Our design: allow one side to merge overlaps or mark them as same, and ensure our matching tries to consolidate by unique domain.
	•	Partner Drops/Changes Data Access: If a partner disconnects their CRM or revokes access mid-process, we need to handle gracefully: perhaps freeze the last known overlaps, notify the other side that partner data is unavailable. Data retention policies need to respect if partner leaves (probably we delete their data after a grace period). Also, if a partner updates an account (e.g. status changes to Closed Won), our sync should update overlaps in near-real-time.
	•	Privacy and Approvals: Edge: a partner might accidentally share too much data or want to revoke a specific account from overlap. We should allow “hiding” an overlap (if for some reason they don’t want to pursue it or it’s sensitive – perhaps a simple checkbox “remove this overlap from partner’s view”, which essentially means “we are not interested” – ideally both have to agree to hide it if it was already visible). Also, user-level access: maybe not every sales rep at a company should see all overlaps – maybe only their own accounts. Our permissions will handle that by linking overlaps to account owners and optionally restricting visibility to owners plus partner managers.
	•	Different CRM Structures: One partner might not use Accounts (maybe only deals or leads). Or a partner might want to share opportunities overlaps rather than accounts (like find where a prospect in my pipeline is a customer of yours). We need flexibility to map different object types. MVP assumes account<->account mapping; an edge case where this doesn’t align can be deferred to later (or handled via CSV upload if needed for MVP).
	•	Time lag and sync conflicts: If both partners update a status on a shared deal in their respective CRMs differently, whose data do we show? Our approach: show each side’s data in separate columns (so no direct conflict). For joint objects like a note, the latest update persists. We’ll implement optimistic concurrency or locking on such shared records to avoid edit collisions (rare in MVP because not editing same fields from both sides except maybe “notes”).
	•	Error handling: If a CRM sync fails (API limit or network issue), we need to alert the user and retry. If Slack message fails (channel not found), log it in an admin console. Essentially, robust logging and user-facing error messages for all integration actions will be needed (especially by V1).

Acceptance Criteria (overall) for each release:
	•	MVP is accepted when a pilot customer can onboard two partners and go through an intro workflow resulting in at least one meeting, with both users primarily using our platform (not resorting to manual spreadsheets or emails outside it). Also, all critical data security requirements (no unintended data leak, proper access control) are verified.
	•	V1 acceptance will include scaling to ~5-10 partnerships per customer, with hundreds of overlaps, without performance issues; automation rules demonstrably saving time (maybe a measured reduction in manual pings); and at least one success story of a closed deal traced through the platform – proving end-to-end value.
	•	V2 success will be measured by broader adoption and the platform’s ability to handle complex scenarios (multi-partner deals, advanced analytics) and possibly by customer willingness to expand usage (net retention, upgrades).

Django + HAT Architecture

We will implement the platform on a modern “HAT stack” – Django (Python) for the backend, Postgres for the database, and HTMX + Tailwind CSS for a snappy server-driven UI. This stack gives us productivity and a lightweight interactive front-end without a complex SPA, ideal for the dashboard-style and form-driven interactions in our app.

System Architecture Overview

At a high level, the system will be a multi-tenant web application. Each company (organization) is a tenant whose data is isolated at the application level (and optionally at DB level via schemas or partitions). We will use a modular Django app structure, roughly separated by domain areas:
	•	Core (Organizations & Users): Org model, User model, authentication, roles/permissions, audit logs.
	•	Data Integrations: handling external CRM connections, data import jobs.
	•	Partners & Sharing: models for Partnerships, policies, and overlap logic.
	•	Collaboration/Workflows: overlap records, requests, notifications, automation rules.
	•	Analytics/Reporting: models or ORM queries for summary metrics (though could be integrated in above models as methods).

Django App Structure: We might create Django apps like organizations, crm_integrations, partners, overlaps, automations, etc., to keep the code organized. Each app contains models, views, and tasks related to its domain.

We’ll leverage Django Rest Framework (DRF) to build some API endpoints (for integrations, extension support), but many views will be rendered HTML pages enhanced with HTMX for interactivity. HTMX allows us to add dynamic elements (like accepting a request without full page reload) by returning HTML fragments from Django views.

HTMX + Tailwind UI flows: We will progressively enhance pages: e.g. the overlaps list – when you click “Request Intro”, instead of a full page, HTMX could fetch the modal form fragment from a Django view and display it. Upon submission, HTMX could swap in the updated row (status changed to “Requested”) without reloading the whole page. This approach keeps the UX smooth and fast, approaching SPA feel but with simpler maintenance.

State management: Mostly server-side (Django session for user auth, and context passed into templates). HTMX will carry session or CSRF tokens as needed. We avoid heavy JS frameworks: any client-side logic will be minimal (maybe just some Tailwind UI components scripts or using Alpine.js for a bit of interactivity in forms if needed).

Data Model Design (Django Models)

The database (PostgreSQL) schema will be designed to support multi-tenancy and efficient overlap queries. Key models (with essential fields) include:
	•	Organization (org) – Represents a company using the platform. Fields: name, industry, etc., and flags like is_active. It will have relationships to Users, Accounts, Partnerships. Each model that is owned by an organization will have a foreign key to Organization (for multi-tenant isolation). We’ll add an Organization.id foreign key to Account, etc., to scope queries. Possibly consider using PostgreSQL schemas per org for extra isolation, but likely not needed initially – a simple org_id field with indexing might suffice.
	•	User – Django’s built-in User model (with extension or a profile model). Fields: username/email, password (hashed), and a foreign key to Organization (if a user can only belong to one org in our design – which is likely, unless we later allow a consultant to have multi-org access). Also a role field or group memberships (e.g. roles: Admin, Manager, SalesRep). Permissions will be derived from role: e.g. Admin can invite partners, SalesRep can view overlaps only for their accounts, etc.
	•	Account – Stores an account (or company/contact) record imported from a CRM for an org. Fields: org (FK to Organization), external_id (ID from source CRM), name, domain, crm_owner (which User or at least the owner’s name/ID from CRM to identify the sales rep), type (customer, prospect, etc.), stage (if relevant, e.g. for prospects maybe pipeline stage). Possibly city/country if useful for matching or insight. We might also store a hashed identifier: e.g. hash_domain = SHA256(lower(domain)) and/or hash_name = SHA256(normalize(name)) – these can be used for join comparisons across orgs without revealing actual names ￼. We’ll index those for fast matching. Each Account record belongs strictly to one org.
	•	Partnership – Represents a partnership relation between two Organizations. Fields: org_from and org_to (FKs to Organization) – we can enforce a convention like the smaller org ID is always org_from to avoid duplicates, or use a separate join table with unique constraint on (org1, org2). Also, status (pending, active, disconnected). Possibly fields for trust settings e.g. share_accounts=True/False, or a JSON field for field-level sharing preferences. We expect partnerships to be symmetrical (two-way), so probably one record per pair with flags each side can control. We might also have a separate model PartnerUserAccess if we allow direct user-to-user connections, but likely not needed – we mediate via orgs.
	•	Overlap (or Match) – Represents a matched account between two partners. One design: do not store overlaps persistently but compute on the fly with queries. However, for performance and to attach collaboration data, it’s better to store them. Fields: partnership (FK to Partnership), account_a (FK to Account), account_b (FK to Account). Also perhaps store some context: e.g. last_synced timestamp, and any aggregated info like “account_a_stage” and “account_b_stage” (denormalized for quick display so we don’t have to join back to Accounts each time). Unique constraint on (account_a, account_b). Note that an account from org A could match multiple of org B if duplicates; our fuzzy logic should avoid false multiples, but deterministic by domain should be one-to-one ideally. Overlap records are created/updated by a background job when data changes. We also add fields for collaboration status: e.g. intro_requested (Boolean), intro_accepted (Boolean or date of acceptance), or we break those into a separate Collaboration model as below.
	•	Collaboration (CoSellDeal) – Represents a co-selling thread on an overlap. We might merge this with Overlap or keep separate. Initially, an Overlap with no action is just data. Once someone initiates an intro or any collab, we could have a Collaboration record. Fields: overlap (FK), status (e.g. Intro Requested, Intro Accepted, In Progress, Closed Won, Closed Lost), created_by, maybe deal_amount, notes. If we allow multiple “deals” per overlap (say overlapping account yields two different opportunities), then having a separate model makes sense. But likely one overlap -> one collab flow at a time (you could reopen later if a new opportunity arises). We’ll implement minimal: use Overlap as primary and maybe an Overlap.status field for intro status in MVP, then evolve to separate model in V1 when pipeline tracking is added.
	•	IntroRequest – Alternatively, specifically model the intro request as an event. But it may be overkill – a Collaboration with status can serve. If needed: Fields: overlap, from_user (who requested), to_user (target partner user, or null if any partner manager can accept), message, status, created_at, responded_at. We can derive a lot from Overlap’s state though.
	•	AutomationRule – Stores a user-defined rule. Fields: org, name, trigger_type (enum: e.g. “NewOverlap”, “StageChanged”, “Scheduled”), trigger filter details (could be JSON or separate related model for conditions), action_type (enum: “NotifySlack”, “CreateTaskSFDC”, etc.), action config (JSON or fields like target channel, message template, etc.), is_active. We’ll need a rules engine to evaluate conditions; for simplicity store something like a JSON of conditions or a Python pickled lambda for advanced, but likely just code common ones. Might integrate with Django’s signal framework: e.g. after saving an Overlap, fire signals that check rules. We’ll process rules in background jobs to avoid slowing web requests.
	•	Notification – Could have a model to track notifications sent (for audit and for enabling in-app notification feed). Fields: user, message, timestamp, channel (in-app, email, slack), is_read. This helps build the notification UI and debug if emails were sent, etc.
	•	IntegrationCredentials – For each Org, store credentials/tokens for connected integrations (Salesforce, HubSpot, Slack, etc.). Fields: org, integration_type (e.g. ‘salesforce’, ‘hubspot’, ‘slack’), auth_info (encrypted JSON containing tokens or keys), maybe metadata (like instance URL). This is sensitive information – we’ll encrypt these fields in the DB (using Django’s encryption library or at least mark as sensitive).
	•	EventLog – (Optional) A generic event log for important actions (for audit and possibly feeding analytics). Fields: org, event_type (like ‘OVERLAP_FOUND’, ‘INTRO_REQUESTED’), details (JSON), created_at. This can feed an activity feed or just for internal analysis.

We will also use standard Django models like django_celery_results if using Celery for tasks, etc., but above are core domain models.

Background Jobs & Tasks

Certain operations will be handled by background workers (e.g. Celery or Django Q) to improve responsiveness and handle periodic work:
	•	CRM Sync Jobs: Once a CRM integration is set up, a periodic job (say every X hours or triggered via webhook if available) will fetch new/updated accounts from CRM. For Salesforce, we might use their Bulk API or streaming API (though streaming requires push topics; for MVP maybe periodic poll). For HubSpot, webhooks can notify changes. The job will upsert into our Account table. We will have a task queue for “sync_org_crm(org_id)” that can be scheduled. This job might also directly compute overlaps if we want immediate updates.
	•	Overlap Computation: If data volumes are large, we’ll offload the heavy lifting of matching to a background job. For MVP, since we limit scope, we might compute on-the-fly for each partnership on view. But better: whenever Org A or Org B’s accounts change, enqueue a job “compute_overlaps(partnership_id)”. That job will fetch new/updated accounts and find matches. Complexity note: We can also do a full recompute nightly for each partnership for simplicity initially (and optimize later). We must ensure computations scale (see next section for complexity math). The result of this job is updating the Overlap table: insert new overlaps, flag or remove ones that no longer match (if an account is deleted perhaps).
	•	Notification & Automation Execution: When certain events happen (a new Overlap, an Intro accepted, etc.), the system will enqueue a job to process automations. For example, a rule “Slack alert on new overlap” would translate to a background task that formats the Slack message and calls Slack API (so that our web request that created the overlap isn’t delayed). Similarly, sending emails for intro requests, etc., can be done async. We will use Celery’s retry mechanisms for reliability (e.g. if Slack API fails due to rate limit, retry after a backoff).
	•	Cleanup & Maintenance: Routine jobs like removing data for a churned customer after 30 days (if required), or compressing old logs, etc., will run on schedule.

Using Django Celery Beat, we can schedule periodic tasks (like “sync all active orgs every 24h”). We’ll design tasks to be idempotent or track state (e.g. store last_sync_timestamp for incremental sync).

Permissions & Multi-Tenancy Security

We treat Organization data isolation as sacrosanct: no data from one org should be accessible by another unless through an explicit partnership and within the overlap scope. At the Django level, we’ll enforce this by scoping querysets by org_id for all model access in views. We might implement a custom manager or middleware to auto-filter by current user’s org for convenience. E.g., Account.objects.filter(org=request.user.org) on all queries, or use Django’s “django-guardian” or simple checks in every view.

For cross-org data in a Partnership: when viewing overlaps, the view code will ensure that the user’s org is part of that partnership – otherwise 403. Each Overlap record inherently belongs to a Partnership which links two orgs; so if request.user.org matches partnership.org_from or org_to, they’re authorized to see that overlap, otherwise not. Additionally, we might store limited partner data in Overlap to display to the other side. e.g. Partner’s account name might be hashed or partially hidden until they accept partnership. However, since both agreed to partner, we assume they consent to see overlap names by default (maybe not full details like internal notes).

Role-based permissions: Within an org, not everyone should do everything:
	•	Admin/Partner Manager: can invite partners, configure sharing, see all overlaps.
	•	Sales rep role: can only view overlaps for accounts they own (we have owner info on Account). We implement this by filter: e.g. Overlap view filter where either overlap.account_a.owner = current user OR if multiple owners allowed, maybe allow any internal user to see overlaps (depending on org preference). Likely in early version, we’ll let all internal users see all overlaps to encourage collaboration (the data isn’t super sensitive internally). But we will hide partner’s entire account list – only overlaps show.
	•	Also, Admin can set who receives partner requests (maybe by default the Partner Manager handles them, not every user).
	•	Ensure that users from partner org can’t access anything beyond overlaps and associated collab info – they obviously can’t query our DB for the other’s accounts except through overlaps.

We’ll use Django’s auth system plus possibly simple custom decorators on views. For API endpoints, enforce token’s org matches any requested resource’s org.

Data encryption: Sensitive fields like contact info shared in an intro or integration credentials will be encrypted at rest (Postgres supports column encryption, or we can do at application level using libraries). For example, OAuth tokens in IntegrationCredentials – we’ll encrypt those before saving.

Compliance: We will follow GDPR/CCPA guidelines as data processors. The platform is inherently about sharing some personal data (account business contact likely includes personal info). We thus must provide ways to handle data deletion requests, etc., but that’s more an operational concern.

APIs and Integration Interfaces

While our primary UI is web, we will expose a set of RESTful APIs (or GraphQL if needed, but REST+JSON is fine) for certain functionalities:
	•	Incoming Webhooks from integrations: e.g. if Salesforce can call us when a record changes (via a middleware app) or HubSpot webhooks on contact update – we’ll provide endpoints like /api/webhook/hubspot to receive these. These will verify source (HMAC or token) and then enqueue sync jobs for the relevant org.
	•	Our own REST API for third-party or extensions: e.g. the Chrome extension will call an endpoint /api/lookup?domain=acme.com with the user’s auth token to get JSON of overlap info. Similarly Slack slash command will hit an endpoint on our side.
	•	Internal API: Our front-end might use AJAX (HTMX) to get HTML fragments; those are served by Django views. If we use DRF for some pages (like maybe the partner dashboard graphs via AJAX), we’d have JSON endpoints for data.

We will version these APIs and secure them with token auth (each user could have an API token for extension usage, or use session cookie if extension runs with user logged in web – might need token for external contexts).

Integration-specific logic:
	•	Salesforce: Likely use OAuth 2.0 web flow to get refresh token. Use simple-rest (requests) or a library like simple-salesforce in Python to fetch accounts. We might initially support read-only; by V1 support writing back via their REST API (which requires enabling API perms on their side). Also consider using Bulk API v2 for large data sets to improve sync speed.
	•	HubSpot: Use HubSpot’s API (they have a nice client library too). HubSpot provides webhooks for certain events which we can subscribe to (requires a HubSpot app setup). We’ll use incremental sync by storing a timestamp of last sync and fetching contacts/companies updated since then.
	•	Slack: We’ll create a Slack App for our platform. This involves OAuth to Slack to get a bot token. We’ll implement at least two features: posting messages to channels or users (needs chat:write scope), and a slash command (needs commands scope) or Slack events if we want interactive buttons. Slack integration will be relatively straightforward: our backend can call Slack’s API (via HTTP POST) to send messages or we can install an outgoing webhook for slash commands. For example, for notifications, Slack’s app directory listing mentions “Crossbeam data into Slack Connect channels for shared opportunities” ￼ – we’ll allow connecting a Slack channel to a specific partner (Slack Connect is cross-org channel; we can guide users to create one and then our app can post overlap alerts there). This is a differentiator: it keeps partner communication within Slack if they prefer. Our Slack integration should allow mapping a partnership to a Slack Connect channel (store channel ID in Partnership model perhaps). Then, when an intro request is accepted, we could automatically post a message in that Slack channel: “✔️ PartnerCo accepted intro for Acme Corp – meeting scheduled”.
	•	Chrome Extension: We’ll publish a Chrome extension that includes a content script to detect the domain context (like reading the URL or page content for a company name). It will call our REST API. Auth could reuse session cookies if user is logged in on web (or we generate an API token the extension stores). The extension might need to open our web app for some actions (like “click to request intro” might just navigate them to the overlap page). We’ll ensure CORS is configured to allow the extension or use a background script with proper domain.
	•	Other potential integration: Email integration (maybe send an intro email directly, though often users prefer to handle intros personally or via calendar). Possibly calendar integration (to schedule joint meetings). Those are lower priority.

UI/UX Flows

We aim for a clear, uncluttered UI given our mid-market user orientation, enhanced with Tailwind CSS for modern design and HTMX for fluid interactions. Some key UI flows:
	•	Onboarding Flow: After sign-up, a step-by-step onboarding: (1) Connect your CRM (or upload CSV if not ready). (2) Invite a partner (or skip to explore demo data). (3) Once partner joins, auto-navigate to overlaps page. This guided flow will be implemented via either a single page with sections showing progress or separate pages with a progress bar.
	•	Overlaps Dashboard: A page showing a list of partners (sidebar or dropdown to select partner), and the overlaps table for the selected partner. If no partner selected yet, show a call-to-action to invite a partner. The overlaps list can be loaded via normal request or via HTMX swap when selecting a partner. For each overlap row, when clicking it or an expand arrow, show more details (e.g. fields from both CRM records) possibly via HTMX fragment. The user can then click “Request Intro” on that detail panel. That triggers an HTMX call to load the form. After submitting (which creates a request record and sends notifications), the page could update that row to indicate “Intro requested on 2025-05-01, waiting for Partner”.
	•	Partner Invitation UX: Possibly a modal or separate page where you enter partner info. Once invited, that partner appears in list as “Pending invite (Resend?)”. When partner accepts (signs up), status flips to Active and overlaps start populating. Might also have a “Partner Directory” concept in future (to find existing users on the platform) but MVP invites by email.
	•	Notifications UI: Use Tailwind UI components for a dropdown or sidebar showing notifications. Each clickable to jump to relevant record. Also possibly highlight the nav menu items if something needs attention (like a badge on “Requests” if there are pending incoming partner requests).
	•	Automation UI: Could start simple: toggles in settings (like checkboxes: “✅ Notify account owner on new overlap” etc.). In V1, evolve to a form where user selects from dropdowns (Event: [New Overlap] Filter: [Partner = XYZ, Our Stage = Opportunity, Their Stage = Customer] Action: [Slack alert to #channel]). We can simplify by offering common templates (less free-form to avoid user confusion).
	•	Error feedback: Use HTMX swaps to display error messages inline (e.g. if CRM auth fails, show a red alert box without leaving page). Also ensure forms have client-side validation (use Django forms which include error handling, or some lightweight checks with Alpine.js).
	•	Tailwind styling: We’ll create a clean, professional look – perhaps similar to modern SaaS dashboards. Tailwind will help maintain consistency and allow rapid UI changes. We’ll likely incorporate Tailwind components for tables, modals, forms. Use responsive design so that main screens (overlaps) at least work on a tablet or smaller device if needed, although mobile usage might be low for such an app – still, we’ll ensure critical views (notifications, a quick lookup) are accessible on mobile browsers.
	•	Internationalization: likely not needed at start (English only). But keep it in mind if expanding globally.

The overall architecture is designed for rapid development and iteration – Django gives us quick CRUD scaffolding for models, and HTMX enables adding dynamic behavior without rewriting frontend from scratch. Postgres provides reliability and powerful queries which we need for the overlap computation.

Next, we address performance considerations of those overlap queries and scaling.

Postgres Scaling & Overlap Computation

Finding overlapping accounts efficiently is central to our platform. We need to ensure our Postgres schema and queries can handle potentially large data volumes as we grow (each org could have thousands to tens of thousands of accounts, and many partnerships). We analyze complexity and outline scaling strategies:

Overlap Computation Complexity

If one organization A has N accounts and partner B has M accounts, a naive comparison would be O(N×M) – comparing every account from A to every from B. This becomes infeasible as N and M grow (e.g. 50k × 50k = 2.5 billion comparisons). Instead, we leverage database joins on indexed keys to achieve much better performance.

Our matching strategy: use a common key such as normalized domain name or hashed account name. For deterministic matches:
	•	We will compute a normalized_name or domain for each Account. With an index on this field, an overlap query between two orgs becomes:

SELECT a.id, b.id 
FROM account a 
JOIN account b 
  ON a.domain = b.domain 
WHERE a.org_id = A AND b.org_id = B;

This is roughly O(N log N + M log M) to index plus O(K) to merge results (where K is number of matches). Essentially, Postgres will use the index on domain to find matches quickly ￼. This should be very fast for exact matches, even for tens of thousands of rows, as long as the domain values distribution is reasonable. We expect many accounts have unique domains, so the join will primarily compare hashes.

	•	We might store a hash of domain to avoid leaking the actual domain in intermediate operations. For example, hash_domain = SHA256(domain). Then match on hash. Both sides would have to trust our server to not misuse data, but hashing ensures that if one side somehow saw the other’s keys without context, it’s not human-readable. (However, hashing does not fully prevent brute force if domain space is small; but domain space is large enough and we can salt the hash for extra safety).
	•	For fuzzy matching (later), we might use an approach like creating a trigram index on account names (Postgres pg_trgm extension). Then find candidates with similarity above threshold. This is more intensive, but we can constrain it by only applying to accounts that didn’t match on domain first.

Pre-computation vs On-the-fly:
	•	MVP approach could compute overlaps on-the-fly when user views a partner (doing the join query above). If each org has up to, say, 5k accounts and maybe a handful of partnerships, this is okay (a join of 5k×5k is fine with an index). But if some have 100k accounts, that join might still be fine (100k join 100k on indexed key is not too heavy for Postgres if memory and indexes are good, likely sub-second if domain distribution is indexed and not too many duplicates).
	•	However, as we scale to many orgs and frequent updates, computing on-the-fly repeatedly is redundant. So we lean towards storing overlaps in an Overlap table via background jobs. This allows:
	•	Faster page loads (just select from Overlap where partnership=X).
	•	Ability to attach collaboration state to overlaps (requests, statuses).
	•	But it requires maintaining these records on data changes.

We should consider the number of Overlap records: in worst case if two large orgs share a lot, it could be thousands of overlaps per partnership (e.g. if two companies both sell to Fortune 500, they might have 200 overlaps). But it’s unlikely to be millions between any two typical partners (if it is, they basically have same customer base and likely knew that). Many pairs might have only dozens.

Network effect scenario: If we have, say, 100 organizations and each with ~10 partners, that’s 1000 partnerships. If each org has ~10k accounts, worst-case if everyone overlaps with everyone strongly, it could be a lot of Overlap entries globally (but realistically, not every company overlaps with every other significantly).

We will monitor performance: Postgres can handle millions of rows easily with proper indexing. We might partition the Overlap table by org or by partnership to keep sections small (like partitions keyed by org_id or partnership_id range), which can help with maintenance and possibly query performance by pruning partitions. Partitioning by org_id (the smaller of the two org ids in the pair) could mean all overlaps involving that org are grouped – queries by partnership (which filters by both orgs) would hit at most 2 partitions.

Indexing Strategy

Accounts table:
	•	Index on org_id (as part of composite indexes) because we frequently filter by org.
	•	Index on matching keys: e.g. domain (text) – use B-tree index (which handles equality well). If using hashed domain, definitely index the hash column (perhaps a Hash index or B-tree on the hash value).
	•	If we do a lot of name similarity, create a GIN or GIST index for trigram similarity on name (using the pg_trgm extension), which allows %% ILIKE or similarity queries to use index.

Overlap table:
	•	Primary key index on id.
	•	Unique index on (partnership_id, account_a, account_b) to avoid duplicates.
	•	Index on partnership_id (since queries will be like WHERE partnership_id = X to list overlaps).
	•	If we need to query overlaps by org (like total overlaps across all partners for an org’s dashboard), index on org_id as well. We might include both orgs in each overlap row for easy querying. Actually, storing org_a_id and org_b_id in Overlap could be redundant because partnership can be joined to get orgs. But adding for convenience might help queries like “all overlaps for org 5” (we’d do WHERE org_a_id=5 OR org_b_id=5). We can maintain those fields denormalized in Overlap for speed at cost of duplication.

Others:
	•	Partnership: unique constraint on (org1, org2) to avoid duplicates. Index on org1 and org2 for lookups (like find all partners of an org).
	•	Automations: maybe index on trigger type or so, but volume will be low.

Query performance targets

We want key operations to feel instant for the user:
	•	Viewing overlaps: Ideally <2 seconds to load the overlaps page for a partner with up to a few hundred overlaps. With indexing and caching, that’s achievable. If we precompute, it’s just a select of at most a few thousand rows, which is fine. If on-the-fly, the join should also be sub-second with indexes given moderate sizes and a single partnership query.
	•	Searching/filtering overlaps: If user types a search (HTMX request to filter), we should search on account name efficiently – might need a text index, but given the relatively small result sets per partner, we can also do client-side filtering if already loaded in DOM or just query with ILIKE with index support.
	•	Sync operations: If an org has 50k accounts, pulling that from Salesforce might take e.g. 1-2 minutes via API initially. That’s acceptable as a background job. Overlap computation of 50k vs 50k maybe might take a few seconds in DB. We can aim for initial full sync under, say, 5-10 minutes, which is okay if done once or in background. Then incremental updates handle smaller chunks quickly.

Partitioning & Scaling Out

As the number of accounts and overlaps grows, we have options:
	•	Vertical scaling: beefier Postgres instance with more RAM to keep indexes in memory. Use connection pooling to handle many requests.
	•	Read replicas: Could offload heavy read queries (like someone generating a big report) to a replica, though our load might be fine on one DB for a while.
	•	Partitioning: Partitioning the Account table by org_id might be considered if we have some orgs with huge data which we rarely join with others. However, cross-partition joins are tricky. Partitioning Overlap by org or by partnership as noted can help to keep partitions manageable (and possibly even place them on different disk volumes if needed).

Given Postgres 15+ can handle hundreds of millions of rows with proper indexes, we likely won’t hit DB scaling issues in early stages (first 6-12 months). But we plan ahead:
	•	Large org scenario: If a Fortune 100 company signs up with 1 million accounts in CRM, how do we handle? We would likely not sync all – maybe only a subset relevant to partnerships. We could implement sync filters (e.g. only sync customers, not every lead). If needed, for such large data, consider a different storage for matching (like an inverted index or external big data match service), but that’s beyond initial scope. Perhaps we could push heavy matching to an offline batch (like export hashes to a file and use an external tool or something) if absolutely needed, but ideally Postgres suffices.
	•	For now, our performance target: support orgs with up to ~100k accounts each and maybe up to 20 partnerships, with each page load or job finishing in a few seconds. This covers majority of mid-market use cases.

We will also use EXPLAIN on our join queries as we test with sample large data, and add any needed indexes (like if domain is sometimes null, ensure we handle that, etc.). We might add a separate matching on email addresses if needed (some companies might want to match contacts by email domain – but since we are focusing company account mapping, domain covers it mostly).

Index maintenance & storage: We must be mindful that indexing hashed values (like SHA256) produces a fixed 32-byte value; B-tree index on that is fine. If we had, say, millions of accounts, index size might be large but manageable. We should also consider indexing combination keys (maybe domain+country if that helps avoid false matches like same domain in different countries – but domain itself is usually global).

Data retention: To scale, we might not need to keep overlaps that are no longer relevant. E.g., if a partnership is removed, we can delete those overlaps. Or if an account is deleted or changed such that it no longer overlaps, we can remove that overlap record to save space (rather than keep history – unless we want historical analytics, but likely not at account level, we can aggregate historical metrics separately).

In summary, our approach leverages Postgres’s strength in relational joins and indexing. We’ll monitor query times and add optimizations (like materialized views or partitioning) if needed as we approach scale limits. But given the initial target market (mid-market co-selling, not an extreme big data scenario), our design should comfortably handle the expected loads.

Matching & Identity Resolution Model

Accurate matching of accounts between partners is crucial – too strict and you miss opportunities, too loose and you waste time on false matches. We implement a two-tiered matching system: deterministic for exact or unique identifiers, and probabilistic (fuzzy) for those tricky cases where data isn’t perfectly aligned.

Deterministic Matching

Keys used: Wherever possible, we use reliable unique identifiers:
	•	Website Domain: This is our primary key for company matching. If both orgs have a field like “website URL” or email domains for their accounts, and those match exactly, it’s a strong indicator of the same company ￼. We’ll normalize domains (lowercase, remove www. prefix, etc.). This catches things like “acme.com” on both sides. We consider domain match deterministic because companies generally have one official domain; collisions (two different companies sharing a domain) are rare (except multi-tenant domains, e.g., everyone uses gmail.com – but those are usually individuals, not companies).
	•	External IDs: In rare cases, two partners might integrate around a common third-party ID (e.g. DUNS number or some industry registry). If available, we could match on that. Not MVP, but V2 could incorporate external enrichment (like matching through a data provider – e.g. Crunchbase IDs if both have them).
	•	Exact Name Match: If domain is missing, an exact match on normalized company name (after stripping punctuation, case, etc.) can be used deterministically only if the name is reasonably unique. But company names can collide (e.g. “Skyline Solutions” could be many firms). So exact name match will be considered but perhaps flagged if ambiguous. In MVP, we might treat exact name as a match, but in fuzzy phase we will refine with context.
	•	Email domain for contacts (if we later match individuals): For instance, if we go down to contact-level overlaps (maybe far future), matching contact email domains can link contacts to accounts.

We also incorporate contextual deterministic rules: For example, if one side’s account has the same street address or phone number as another’s – that’s a strong sign (though phone can vary format). Initially, we likely won’t have address for all, but if CRMs provide it, we could consider it in later versions.

Scoring/Confidence for deterministic: Actually deterministic means we consider it a match without doubt. Domain match will be considered confidence 1.0 (or 100%). We will still allow users to override if something looks off (maybe two subsidiaries using same domain? Rare).

Data normalization:
	•	Names: Remove legal suffixes (Inc, LLC, etc.) for matching purposes, because “Acme Inc.” vs “Acme LLC” should match. We’ll maintain a list of common suffixes to drop.
	•	Remove punctuation, etc.
	•	Use ASCII folding to avoid accent differences if any.
	•	Domain: ensure format consistent (we may extract root domain if subdomains are used inconsistently, although subdomain might indicate different divisions – but typically CRM would store main website).

Collisions handling: If two different accounts in Org A map to one account in Org B (because of a duplicate or they intentionally split an account into two records), then multiple overlaps could point to the same partner account. That’s okay logically, but we might want to flag it. Possibly we will merge on our side for clarity (“Org A accounts X and Y both match Org B’s Z – you might have a duplicate or different contacts of the same company”).

Probabilistic Matching

For cases where deterministic keys fail (e.g., one side has no domain on a record, or uses slightly different naming), we introduce fuzzy matching in V1:

We will calculate a similarity score for potential matches:
	•	Name similarity: Using e.g. trigram similarity or Levenshtein distance on the company name field ￼. We’ll likely utilize Postgres’s trigram extension: a similarity score between 0 and 1. e.g. “Acme Corporation” vs “Acme Corp.” might yield >0.9 similarity. We’d consider above, say, 0.8 as strong match provided no other candidate is similarly high (to avoid confusion).
	•	Location (if available): If we have city/state, we can boost matches that share locations. E.g. “Acme, LLC (New York)” vs “Acme Corp (California)” – if two records have identical name but different location, maybe they are different. If location is same, increases likelihood it’s same.
	•	Industry or other attributes: If CRMs have industry codes or sizes, matching those could help but might be too inconsistent to rely on. Possibly ignore in automated scoring, but might use as secondary info for manual review.
	•	Contact/Email cross-reference: If in the future we have contact data, e.g. an email on one side belongs to a domain of the other’s company, we could use that. For now, not likely in MVP.

We will likely implement a small scoring algorithm: for each account in Org A that didn’t deterministically match, compare with all accounts in Org B (or use index to find name-similar ones):
	•	Compute name similarity score.
	•	If above threshold (like 0.8), consider it a candidate.
	•	If also same country or state, + some score; if different, maybe - some score.
	•	Possibly consider partial domain: e.g. one has domain “acme-inc.com”, other no domain but name “Acme Inc” – we might consider that a strong match if the names align.
	•	For individuals (if any, not likely here) you might match phone or email, but for companies that’s not relevant aside from domain.

We will set two thresholds:
	•	High confidence threshold (e.g. 0.9): above which we auto-match as if deterministic (maybe after ensuring uniqueness).
	•	Possible match threshold (e.g. 0.7): above which we flag for user review in the UI.

We’ll present “Suggested overlaps” that users can confirm or dismiss. For each suggestion, show fields from both records to help decision (name, location, maybe key contacts if any).

Human in the loop: Partner managers can validate fuzzy matches. If both partners have to agree (maybe one suggests and the other confirms?), but that could slow things. Perhaps if one side confirms it, it becomes an overlap visible to both, with an indicator that it was a fuzzy match.

We also plan to gather training data from these confirmations (in long run, maybe feed into an ML model to improve suggestions).

Ongoing evaluation and improvement

We will continuously evaluate match accuracy with the following plan:
	•	Pilot testing with real data: We’ll use a dataset from our pilot users (with permission) or a synthetic set to run matching and then manually verify if the matches are correct and if any obvious overlaps were missed. Key metrics: precision (of the overlaps we show, how many are true overlaps) and recall (how many true overlaps did we find vs total true ones known). We aim for very high precision (we don’t want false overlaps as those erode trust) – ideally >95%. Recall can be lower initially but should improve with fuzzy logic (target maybe ~85% with fuzzy).
	•	User feedback mechanism: In the UI, allow users to mark “Not a match” if they see a wrong overlap ￼ (e.g. two different companies with similar name matched incorrectly). Also allow “Add a missing overlap” where they can search partner’s accounts and link one if our system missed it. These corrections can be logged and later fed back into our rules (e.g. add that name pair to a blacklist or refine thresholds).
	•	Scoring threshold adjustments: Based on feedback, we may adjust the fuzzy threshold. If too many false matches come at 0.8, we bump threshold up, etc. We might also per customer allow them to set conservativeness (some might want “only show very sure matches” vs others “show me everything possible and I’ll sort”).

Identity Resolution Beyond Companies

While initially focusing on company/account matching, identity resolution principles apply if we ever include contacts or leads:
	•	We’d then match individuals by email (exact) or name+company combos, etc. But that’s future scope. It’s worth noting that our architecture could extend to contact-level overlaps (e.g. find if the same person is known to both companies – for partner referral perhaps). If we go there, we’ll apply similar mix of deterministic (email) and fuzzy (name similarity + account match context).

Data Quality and Preprocessing

We will invest in basic data cleansing as part of import:
	•	Remove common noise like “(Inactive)” or customer codes appended to names if any.
	•	Ensure no trailing spaces, etc. This makes matches more effective.
	•	Possibly integrate a library or service for company name normalization (like OpenCorp or similar). But to start, our own normalization functions should do a decent job.

Example Scenario:
	•	Org A has “International Business Machines Corp.”, no domain listed. Org B has “IBM” with domain “ibm.com”. Deterministic domain doesn’t catch (A has none). Name similarity between “International Business Machines” and “IBM” is low at string level. But we might have a known acronym matching (could incorporate a dictionary of common company acronyms if needed). Without that, our system might miss this match initially. To address this edge case: we could use a third-party enrichment (like if we pull a known alias list or if A’s CRM may have ticker symbol etc). In MVP, we may miss such cases, but as we gather data, we might add an alias mapping (perhaps let user manually match those or use our vertical knowledge if focusing on tech you know IBM = International Business Machines).
	•	Org A “Google LLC”, Org B “Google, Inc.” – our normalization drops LLC/inc, making both “Google”, exact match -> overlap. Good.
	•	Org A “McAfee” vs Org B “McAfee, LLC (acquired by Intel)” – names match on “McAfee”, domain likely same – overlap found.
	•	Org A “Apple” vs Org B “Apple” – domain apple.com matches, but consider if one had domain “appleinc.com” vs “apple.com”. That would be a miss if we only use domain. But name exact match would catch and we’d likely consider it match (though “Apple” could also be a small business named Apple Consulting, but domain difference might have flagged that anyway).
	•	Ambiguous case: Org A “ACME Corp” (sells anvils in USA), Org B “ACME Corp” (different ACME that sells software in UK). If both are named “ACME Corp” with no domain info, our system would match them erroneously by name. This is a risk of deterministic exact name. We might mitigate by noticing that no other data matches (industries differ, locations differ significantly). Perhaps in fuzzy mode, we require either a domain or some corroborating detail for auto-match, and treat pure exact name with conflict as a lower confidence (for user to verify). In MVP, if this occurs, user can mark “not a match” and we may need to introduce an ignore for that pair.

We will make sure the matching process is transparent to users. We can show, for each overlap, why it matched (“Matched on domain” or “90% name similarity”), so they have confidence or can question it.

Summary: Our identity resolution approach starts simple (strict keys) and evolves to sophisticated (fuzzy + user feedback + possibly external data for validation). Over time, our matching accuracy itself can become a selling point if we do it better than others – e.g. catching non-obvious overlaps (which effectively creates more opportunities for them). We will evaluate continuously by measuring false match reports and missed match additions from users, aiming to minimize both.

Automation Engine Specification

One of our distinguishing features is the automation engine that turns partner signals into actions. We’ll design this as a flexible, event-driven pipeline within the Django app.

Event Model

The system will define various events that occur during the co-sell process. Examples of events (with their payloads) include:
	•	NewOverlapCreated – when a new overlap is identified between Org A and Org B. Payload: partnership ID, account info from both sides (e.g. account names, owners).
	•	IntroRequestSubmitted – user from Org A requested an intro on overlap X. Payload: overlap ID, requesting user, target partner org.
	•	IntroRequestResponded – partner accepted/declined. Payload: overlap ID, response, responding user, maybe contact shared.
	•	OpportunityStatusChanged – if integrated with CRM, e.g. an opportunity linked to an overlap changed stage (like moved to Closed Won).
	•	PartnerAdded – a new partner connection established.
	•	Periodic triggers – e.g. a scheduled event “reminder” if something pending too long (this can be modeled as a cron event or timer rather than triggered by data change).

We will implement event capturing by hooking into the relevant code points:
	•	In Django, after saving a new Overlap, fire a “NewOverlapCreated” event (could simply call our automation dispatcher function).
	•	For user actions like intro request, since that’s a view handling the form, after processing, we create an event.
	•	We might implement a small Event Dispatcher that takes an event name and payload, and then checks for any rules that should fire on that event.

We can represent events as simple Python dictionaries or a lightweight class, and route them synchronously to the rules engine which enqueues actions. Alternatively, we log them in DB (EventLog) and a separate worker monitors that table for new events to process (decoupling might be safer to not slow the web thread).

Signal→Action Pipelines (Rules Engine)

The automation engine will allow users to define if-then rules. Each rule has:
	•	Trigger (Signal): defined by an event type and optional conditions on the event data.
	•	Action: one or multiple actions to execute when triggered.

Rule representation:
We could use a simple rules DSL or just store in DB and evaluate in Python. For MVP, we might actually hardcode a few automation behaviors (like always notify rep on new overlap) to test value. By V1, user-defined rules come in.

We anticipate common triggers:
	•	On new overlap (maybe with filters: e.g. partner = X, or our account stage = Y, or account value > $Z).
	•	On intro request events.
	•	On no response (which might require a time delay trigger – e.g. if intro request open >7 days).
	•	On deal closed (to do an automatic thank-you or record in system).

Conditions UI -> underlying logic:
If user chooses conditions like “Partner’s Stage = Customer”, that means in event payload we have partner_account_stage = “Customer”. Our rule engine should check that. We can implement as evaluating a Python expression or a safe templating. Possibly use something like Django-rules￼ or write a small custom evaluator:
We could use Python’s eval on a context dict, but that can be risky (though we control the expressions templates). Safer might be to have pre-defined filter fields like:
	•	event.partner_account_stage == ‘Customer’
	•	event.my_account_stage == ‘Opportunity’

We can design conditions as JSON: e.g.

{ 
  "field": "partner_stage", "operator": "equals", "value": "Customer"
}

with conjunctions if needed. Then our engine can interpret that for the given event.

Action execution:
The actions might include:
	•	Send Notification: (subtype: Slack message, Email, In-app). e.g. Slack to a channel or user.
	•	Create CRM Task/Activity: via Salesforce/HubSpot API. We’d need to have integration credentials and proper permissions. If a rule says “Create Salesforce Task for account owner”, our code finds that account owner’s SFDC ID and uses SF API to create a Task record (with some default text from a template).
	•	Update Field: e.g. update a custom field “Partner Intro Sent” on an Account in CRM or in our DB.
	•	Custom Webhook: perhaps allow posting to an external URL (for teams that want to integrate with other systems).
	•	Internal actions: maybe transition some status in our app.

We will have an Action dispatcher – it takes an action definition and executes it. Execution often will involve an external call (Slack API, etc.), so we will do it in background job as mentioned. We’ll design actions to be idempotent where possible (especially external ones – we might include dedup logic e.g. don’t create duplicate task if one already open for that overlap).

For multi-step pipelines: The question mentions “signal→action pipelines”. Our initial engine may allow one trigger to fire multiple independent actions (like Slack + Email at same time). If we consider sequential steps (like if Action A outputs something that triggers Action B), that may be overkill. Instead, each event can trigger multiple rules. If we need a sequence (like 3 days after intro requested with no response, send reminder), we might model that as a separate event (“NoResponseReminder”) that is scheduled at T+3 days if no response event came in by then. This effectively creates a pipeline across time. We’ll implement that via scheduling events: when an intro is requested, schedule a check 3 days later (persist something like PendingReminder with timestamp). If by then the intro is still pending, fire the “Reminder event” which triggers an email.

In summary, our “pipelines” are relatively simple flows encoded by multiple rules and scheduled events, as opposed to a full BPMN-like process engine. This is sufficient for our needs (common flows are linear).

Job Orchestration & Processing

We will orchestrate automation tasks primarily through Celery workers:
	•	The main web thread captures events, writes to DB or directly calls a Celery task for the rule evaluation.
	•	A Rules Processor task can run: find all active rules of type matching event, evaluate conditions, for each that match, invoke the actions (could spawn separate tasks for each action or do sequentially).

We’ll ensure if multiple rules fire at once, they all execute – they should be independent unless user sets contradictory actions (rare scenario, e.g. one rule posts Slack, another also posts Slack with slightly different message – that’s fine).

For scheduling future events (like reminders), we can either use Celery’s countdown/ETA feature (schedule a task X days later), or store in DB and have a periodic scheduler that checks for due reminders. Celery ETA is straightforward if we trust the worker to be up then. We’ll use that for simplicity – e.g. when intro requested, schedule check_intro_response(overlap_id) for +3 days. That task will see if still no response, then emit a Reminder event or directly send a notification if we decide not to bother with an event at that stage.

Observability and Monitoring

It’s critical to have visibility into the automation engine’s behavior, both for users (so they trust it) and internally (to debug issues):
	•	Audit trail for automations: We will log every rule execution in an AutomationLog (or general EventLog as mentioned). This includes: rule ID, event that triggered, timestamp, actions executed, and success/failure status of each action. This log will be exposed in the UI for admins possibly – e.g. an “Automations History” screen where they can see “Yesterday 10:00 – Sent Slack alert for new overlap with Partner X – Success”. If an action fails (say Slack API was down), mark it failed and potentially retry depending on error.
	•	User notifications of automation: When an automation does something visible (like creates a task in CRM), we might notify the partner manager with a summary (“Task created for AE John on Salesforce” maybe in their notification feed, unless they disabled it – we’ll find balance between transparency and noise).
	•	Error handling: If an automation action fails, we’ll implement retries (Celery does automatic retries if configured for certain exceptions). E.g. if Salesforce API rate limit is hit, log it and try again in a minute. We’ll also catch exceptions so one failing action doesn’t stop others. The user should perhaps be alerted if something critical failed (“Reminder email failed to send”) – likely we aggregate those in an admin console rather than spamming end users.
	•	Performance monitoring: The automation should run quickly – but in heavy event scenarios, we might queue up a lot. We will monitor Celery queue lengths and processing times. If an org connects a partner and suddenly 10k overlaps get created, that could trigger 10k notifications – our engine should handle that gracefully (throttle or batch notifications). Possibly, if an event type is bulk (like initial sync generating many overlaps), we might not notify on each overlap, instead one summary (“500 new overlaps found with Partner Y”). We can implement logic for that scenario to avoid overload.
	•	Security & Guardrails: We will ensure automations cannot cause data leaks. For instance, a rule on “New Overlap” will only ever notify internal people or the partner involved, never some unrelated third party. We restrict actions so you cannot accidentally send partner’s data to another partner. Essentially, triggers and actions are limited to within the org’s context or their direct partner context, which is fine. Also ensure one org cannot create a rule that executes actions on another org’s behalf except as allowed (they can only send messages to their Slack or create tasks in their CRM, not in partner’s systems unless partner explicitly gave that integration for joint stuff, which we don’t do).

Example Automation Use-Cases in our system
	•	Internal Alerting: Org A sets rule: “If a new overlap with Partner X is a customer on Partner side and prospect on our side, notify the account owner via Slack and tag the partner manager.” This translates to event NewOverlap with condition partner_stage=Customer, my_stage=Prospect, action Slack message: “Partner X can intro you to their customer ProspectCo – reach out!”
	•	External Partner Slack Collab: If Org A and Partner B have a shared Slack channel, we can auto-post: e.g. trigger when Org A requests intro -> action: post message in Slack Connect channel “Org A requested an intro to Acme Corp (their prospect which is your customer). Please respond here or in the app.”
	•	Reminders: Trigger if IntroRequestSubmitted and not responded within 5 days -> action: email partner’s PM “Friendly reminder: please respond to the intro request for Acme Corp.”
	•	CRM Sync Actions: Trigger on IntroRequestAccepted -> action: create an “Influence” record in CRM or add a note “Intro with Partner done”. Or if Opp Closed Won -> action: update overlap status & maybe send a “celebrate” notification to both sides or schedule a follow-up QBR item.

These illustrate how the automation engine ties into multiple integration points (Slack, Email, CRM) to drive the process forward without manual intervention, achieving the Execution OS vision. By providing a rule builder, customers can customize it to their workflow, and over time we can share best-practice templates in the app (like a library of recommended automations).

In summary, the automation engine will be a combination of Django signal handling, Celery tasks, and a rules datastore driving conditional logic. It’s a core part of our technical architecture that will be built incrementally – starting with hardcoded critical notifications in MVP (to prove the concept of improved speed) and evolving into a full no-code automation interface by V1.

Integration Plan: Salesforce, HubSpot, Slack, Chrome Extension

Seamless integration with existing tools is essential for adoption. We plan the following integrations, focusing on data sync and real-time collaboration:

CRM Integrations (Salesforce & HubSpot)

Salesforce CRM: Many of our target users (especially upper mid-market and enterprise) use Salesforce. Integration will be via Salesforce’s REST API:
	•	Authentication: Use OAuth 2.0 Web Flow. Admin user from Org grants access via a Salesforce Connected App we create. We store the refresh token securely (encrypted) in IntegrationCredentials ￼.
	•	Data Sync (Inbound): Once connected, we fetch data:
	•	Accounts: Likely sync Account objects (fields: Name, Website, Industry, Owner, etc.). Possibly also Opportunities if we want to know pipeline status for matching (but accounts suffice for overlap, opps used for pipeline tracking later).
	•	Contacts could be pulled if we decide to match contacts, but initially skip.
	•	Use Bulk API if pulling large datasets (this can retrieve thousands of records quickly in batches).
	•	Save last sync timestamp. For incremental updates: We can use the CDC (Change Data Capture) or Streaming API if the client’s SF edition supports it – but that’s complex. Instead, poll: e.g. use SystemModStamp filter “WHERE LastModifiedDate > last_sync_time” each run.
	•	Also consider using Salesforce’s partnerNetworkId if they use SF’s built-in account sharing, but that’s outside scope; we treat it as just data source.
	•	Data Sync (Outbound): For writing back:
	•	Custom Object: We create a custom object “Overlap__c” with fields for Partner Org, Partner Account, Overlap Stage, etc. Our app will create/update these records to reflect overlaps ￼. That lets users build SF reports on overlaps and see them inside Salesforce accounts (via related list).
	•	Tasks/Events: When automation calls for it, we create Tasks (e.g. “Follow up with partner for account X”) assigned to specific users, or update Opportunity fields (like add a checkbox “Partner Influenced”).
	•	We ensure we don’t spam updates: only push changes when something significant changes (intro accepted, etc.).
	•	Data Freshness: We aim for near-real-time. Realistically:
	•	On initial connect, user might wait minutes for data to import depending on volume (we will show a spinner/progress “Importing 15,000 accounts…”).
	•	Updates: we schedule frequent sync. Perhaps every hour by default, and manual “Sync now” button. Key changes like new opportunities closed might not reflect instantly unless user triggers sync or we have webhook.
	•	If user needs instant update, they can click a sync button. Or we can leverage Salesforce outbound messages or Apex triggers to call our API when certain events occur, but that requires setup on their SF, which is heavy. Perhaps in V2 for big clients.
	•	So likely a compromise: periodic polling (every ~15 minutes if API limits allow) for key objects. We can narrow queries (e.g. only fetch accounts that changed).
	•	Handling Limits: Salesforce APIs have rate limits (e.g. 15k calls/24h for many orgs). We must be efficient: use bulk where possible, and combine queries. For example, we might fetch only fields we need. If customer has >50k accounts, bulk is necessary. We might have to implement paging.
	•	Error Handling: If token expires or is revoked, notify user to reconnect. If we hit an API error (like a field not accessible), log and skip that field. Keep integration robust.

HubSpot CRM: Many mid-market firms use HubSpot, which tends to be easier to integrate:
	•	Authentication: OAuth 2.0 (HubSpot API).
	•	Data Sync Inbound: Fetch Companies from HubSpot (equivalent to accounts). Also Deals if needed. HubSpot API allows pulling all companies with pagination, or webhooks when companies change (HubSpot has a webhook system that we can subscribe to specific events like “company property changed”).
	•	We’ll gather Company properties like Name, Domain (HubSpot typically ensures a domain field is set for deduping), Lifecycle Stage, Owner.
	•	HubSpot limits are fairly generous for mid volume, but we still do incremental updates. HubSpot’s API returns a “last updated” timestamp for records; we can store a cursor or rely on webhooks to catch changes. Possibly simpler: subscribe to HubSpot’s webhooks for company creation/update, which will push data to our endpoint within seconds (HubSpot webhooks include object ID and changed fields).
	•	Outbound Sync: HubSpot allows custom properties on Company or engagements. We could create a property like “Partner Overlap (Yes/No)” or even create “Notes” on company timeline about partner events. Or create “Tasks” (HubSpot has engagements) assigned to sales reps. We’ll integrate similar to Salesforce: e.g. when intro accepted, create a task in HubSpot for owner “Follow up with referred contact”.
	•	Or we can integrate via HubSpot workflows: some customers might prefer we simply write data and they use HubSpot’s workflow automation for internal handling. For now, we do minimal: maybe update a custom property and let them decide how to use it.
	•	Freshness: With webhooks, we can get near real-time updates from HubSpot (within a minute or so). We still might do a daily full sync for safety.
	•	HubSpot’s network calls are simpler (JSON payloads, etc.). If any issues, logging them similarly.

Data Consistency & Freshness Guarantees:
Our goal: within a few minutes of a change in CRM, the platform reflects it. For critical triggers (like an opportunity closed that might trigger a partner payout or something), we might expedite via user prompt to refresh or via possible CRM triggers.

We will make it clear in UI when data was last synced (“Data last updated 10 minutes ago. ⟳ Refresh”). So users know if something might be outdated.

Conflict resolution: If two systems update concurrently:
	•	Example: Sales rep marks an opp closed in CRM, partnership manager marks the same overlap as won in our app. We’ll likely treat CRM as source of truth for deal status (so our update would eventually be overwritten by the next sync from CRM).
	•	To avoid confusion, we might restrict editing of certain fields in our app if integrated (like if CRM is connected, we won’t let user manually mark something as closed, rather instruct them to do in CRM).

Security: We handle OAuth tokens carefully, and comply with SF and HubSpot security review processes if needed. Data in transit between CRM and us is via TLS. We only fetch necessary fields, minimizing data exposure.

Slack Integration

We plan a Slack App installation for our product:
	•	Install Process: Org admin clicks “Add to Slack”, goes through OAuth to grant our app access to their Slack workspace. Scopes needed: chat:write to post messages, commands if we implement slash commands, incoming-webhook for a channel-specific integration (or we can use chat:write to any channel allowed).
	•	Notifications to Slack: As per automations or direct features, we send messages. Two modes:
	1.	Direct DM via bot: For personal alerts, we can DM a user (the Slack user must be mapped to our user – we can store Slack user IDs upon integration).
	2.	Channel notifications: The admin could configure a specific channel (like #partner-sales) to post all notifications, or even better, channel per partner (if they use Slack Connect with that partner). Slack Connect scenario: if two companies have a shared channel, our app can be invited to that channel to post overlaps or updates there. We will leverage that if possible (requires both sides to install or one to invite and the other to approve app presence in channel).
	•	Slack messages will be concise and possibly interactive (Slack Block Kit):
	•	E.g. A notification of new overlap might include buttons like “Request Intro” that when clicked, could deep-link to our app or call an API. We might not implement interactive buttons in MVP, but possibly later (needs a Request URL for interactive actions in Slack).
	•	Slash Commands: We can provide /cosell find Acme which triggers our app to search overlaps for “Acme” and return a message in Slack with results (like “Acme Corp is a customer of PartnerX, talk to John (Partner Manager)”). This requires implementing Slack command handling (Slack will issue a POST to our endpoint with the text). We’ll parse and respond with a message. This can be helpful for sales reps who quickly want to check if a company has any partner overlaps without leaving Slack. Crossbeam does something like this ￼.
	•	Data Freshness: Slack integration is event-driven from our side – we send messages when events happen. If a user queries via slash command, the data is as fresh as the last sync in our system (we might trigger a quick refresh if needed, but likely just use current DB state).
	•	Testing and Safety: We’ll test Slack messages to not overwhelm. Possibly group notifications if many overlaps come at once (like “You have 20 new overlaps with Partner Y” instead of 20 separate messages).
	•	Opt-out controls: Users can customize which Slack notifications they get (some might only want certain triggers in Slack vs email).

Chrome Extension

The Chrome Extension (browser extension) will serve as a convenient tool for users (especially sales reps) to access partner insights contextually:
	•	We’ll build it using standard HTML/JS. It will interact with our backend via REST API.
	•	Use Cases:
	•	When browsing a company website or LinkedIn profile, user clicks the extension icon: it shows whether that company matches any overlaps in their partnerships. It might show “Partner info found: PartnerCo has this company as customer, talk to Alice at PartnerCo. Click for details.”
	•	When inside Salesforce or HubSpot web UI, perhaps the extension can detect the company name on the page and display overlap info (though if we integrate natively with those CRMs, extension is less needed there – but not all use our integration).
	•	Identification: The extension will need to figure out the company domain or name from the current page:
	•	For LinkedIn, the URL might contain the company name slug, we can map that to a name.
	•	For any site, we have the domain (e.g. visiting acme.com, domain is obvious).
	•	Possibly allow user to highlight text (like company name) and click extension to search that explicitly if detection fails.
	•	Auth: The extension will require user to log in (maybe piggyback on existing session if our app is open in another tab, but to be safe, a one-time login in extension to get a token stored in local storage).
	•	We’ll provide an API token in user settings that they can paste into extension, or do OAuth flow within extension (open a login popup).
	•	API: We’ll create endpoints like /api/partnerinfo?domain=acme.com that returns a JSON: { overlaps: [ {partner: “PartnerCo”, relationship: “PartnerCo is vendor to Acme (customer)”, note: “Intro available” } ], last_updated: “2025-05-01”} etc. Or if none, indicates no overlap known.
	•	Display: The extension popup will use Tailwind (we can include minimal CSS) to style a small card showing the info. Possibly include a button “Open in CoSell Platform” which navigates the user to our app’s detail page for that account if they want more.
	•	Data Freshness: It uses our DB data, which as long as it’s syncing regularly, is fine. If user needs absolute up-to-minute, they’d refresh through main app. The extension is meant for quick lookup, not initiating syncs (though we could allow a “refresh from CRM” link if needed).
	•	Permissions & Privacy: The extension should be careful not to violate any browsing privacy – we will not capture browsing history, only act on user’s click and current tab domain when invoked. We will list necessary permissions (likely minimal, like permission to make requests to our domain, and read current tab URL on click).
	•	Rolling out extension means going through Chrome Web Store verification; we’ll do that once core is stable.

Data Freshness & Consistency Guarantee

Overall, we guarantee that:
	•	Partner overlap data is updated at least daily for all integrations, and more frequently (hourly or better) for key ones (Salesforce, HubSpot).
	•	Slack and extension rely on our internal data which might be slightly behind live CRM by that interval. But since partners typically don’t make decisions on minute-level data changes, this is acceptable. If there’s a specific need for real-time (like a sales rep just closed a deal and expects it to reflect immediately), we can catch that via CRM triggers or manual refresh.

We’ll communicate to users with timestamps (e.g. “Data from Salesforce as of 5 minutes ago” in UI). And if an action is taken based on possibly stale data (like sending an intro for a deal that actually closed yesterday), ideally the partner would clarify. But as we mature, hooking into more real-time triggers will reduce such cases.

Concurrency and data locking: We must avoid race conditions like:
	•	A sync job runs while user triggers an intro request on an overlap. If sync job removes that overlap (maybe partner removed that account), what happens? Possibly by design we never remove overlaps instantly; we might mark as stale. In unusual cases, we’ll favor not deleting overlaps that have active collaboration. We’ll implement safe checks: e.g. if an overlap’s account on one side no longer exists (deleted), we might keep it around but mark “account removed from source”.
	•	If two users simultaneously invite the same partner from different companies? That’s fine, separate partnerships.
	•	If a partner is editing sharing settings while a sync runs, ensure that the sync job respects latest sharing preferences.

Scalability of integrations:
	•	We will likely run integration syncs in parallel threads (Celery tasks per org). This can hit API limits if too many at once. We might implement a simple scheduler to serially sync big orgs or at least include some jitter to not all hit on the hour. If scaling to hundreds of orgs, we’ll watch API quotas and adjust (maybe request increased limits or have backup of asking user to click refresh if heavy usage beyond allowance).
	•	Possibly integrate with a unified platform like maesn (saw in PartnerStandard listing ￼) or others if we want to offload integration heavy lifting, but likely not initially.

In summary, our integration plan ensures that our platform is embedded in the user’s existing workflow: CRM data flows in and out reliably, Slack keeps their team in sync in real-time, and a browser extension puts partner insights at their fingertips. Combined with the automation engine, this creates a connected system where data moves with minimal friction, and users can trust the platform as their co-sell single source of truth.

Pricing & Packaging Strategy

Monetization will follow a tiered SaaS model, aligned with the value delivered and the scale of use. We’ll design pricing to encourage network growth (so offering a free tier to seed usage) while ensuring we capture value as customers derive ROI.

Tier Design

We propose three tiers: Free, Pro, and Enterprise.
	•	Free Tier (Community Edition): Targeted at small teams or those wanting to try out with one partner. Similar to Crossbeam’s free plan, it lowers barriers to entry and drives network effects (partners can invite partners who can join free) ￼.
	•	Likely limits: e.g. up to 2 partner connections and limited number of accounts (e.g. 1,000 accounts) or only manual CSV upload if not connecting CRM (to limit heavy API use). Also maybe 1 user seat (or small number like 2-3 users) to keep it a small team trial. Automation features might be very limited or not included on free.
	•	The Free tier allows basic account mapping and intro requests – enough to prove value. For example, “Explorer Plan: free for up to 3 seats” was Crossbeam’s model ￼.
	•	Value: it populates the ecosystem with many companies on our network, even if not paying yet. And as they grow or hit limits, they upgrade.
	•	Pro Tier (Growth Edition): This is the main paid tier for mid-market. Pricing model can be subscription per month or per year typically based on either number of partner connections or number of internal users.
	•	One approach: Per partner pricing – e.g. $X per month per partner connection (with maybe some volume discount). But that might disincentivize adding partners, which is opposite of network effect.
	•	Another: Per user (seat) pricing – e.g. $100 per month per user (similar to Crossbeam’s $150/user/mo for Connector Plan ￼). If an org has 5 partner managers, that’s $500/mo. But value is not directly seats-based since many more in org benefit indirectly.
	•	Possibly a combination or base + usage: e.g. $500/mo base includes up to 5 partners and 5k accounts, then an extra fee if exceed.
	•	We need to choose a value metric that scales with usage: likely number of partner relationships managed or size of data could be it. Crossbeam’s higher plan ($4800/mo) is for “unlimited partners” ￼, meaning they partially priced by that metric.
	•	We might start with simpler: e.g. Pro at $1000/month (annual contract ~$12k) for mid-market, which includes, say, 10 partner connections, unlimited users in that org, and up to 50k accounts synced. This is just an example ballpark. We will refine after market testing.
	•	Pro tier includes the full feature set: unlimited overlaps, automation engine, Slack integration, CRM integration.
	•	Perhaps some advanced features reserved for Enterprise tier (like multi-partner overlaps or advanced analytics).
	•	Enterprise Tier: For large companies or specific needs (like dedicated environment, custom integrations).
	•	Pricing: likely custom quotes. Could be $50k-$100k+/year depending on scale. Or as Crossbeam calls “Supernode – custom” ￼.
	•	Differences: Enterprise might include priority support, onboarding assistance, custom security requirements (e.g. on-prem option if needed, though we prefer multi-tenant SaaS), maybe ability to negotiate a per-user or flat enterprise license.
	•	Feature-wise: things like SSO/SAML, audit logs export, more granular permission controls might live here. Also if they need integration with obscure systems or custom automations, we could accommodate under enterprise PS.
	•	Enterprise might also allow unlimited partners and accounts, whereas Pro might cap them (or charge for extra).

Why tiers: This structure aligns with how our competitors are priced. Crossbeam and Reveal both have free and paid plans; Crossbeam’s paid starts around $4,800/mo for broad usage ￼. We want to come in slightly under incumbents to entice switching or adoption, but not so low that we undercut our value.

We also consider a partner program angle: perhaps a network pricing where if both partners are paying, some added benefit (like advanced collaboration features). But that complicates pricing.

Better to price per org usage, not requiring partner to pay – to avoid friction in adoption. Many might use a mix (one paying, one free, which is allowed by our design).

Unit Economics & Cost Drivers

Our costs will scale with:
	•	Data storage and processing: Storing potentially millions of account records and performing matching computations. Postgres and servers costs on cloud (but these are manageable, each mid-size org’s data isn’t huge; the heavy is if we get some with millions of accounts).
	•	API integration usage: Salesforce API calls (Salesforce gives a certain number per day per connected app + per org, but if volume, we might have to handle that carefully; if we exceed we might need to pay for additional API allowance or ask customer to).
	•	Notifications and automation runs: Slack messages and emails (email is cheap via services like SendGrid, Slack API usage is free aside from dev effort).
	•	Support & Onboarding: Our team time – smaller customers get scaled support (help center, community), enterprise expects hands-on (so their price covers solution engineering time).

We want healthy margins (typical SaaS gross margin ~80%). So pricing should far exceed our variable cost per customer:
	•	For example, a mid-market org paying $12k/year might have 10k accounts, 5 users. The infra cost for that might be maybe a few dollars a month in DB and API usage – negligible. Support might be some hours initially. So margin very high.
	•	Our main cost is actually building the product and fixed overhead, which pricing covers once we scale volume.

Value-based Pricing Rationale:
We align price with ROI:
	•	If our platform helps close just one extra deal of say $50k, that already pays for many months of service. Crossbeam touts 5-month payback on investment ￼, we should aim similar or better (like ROI within a year at most).
	•	We can model ROI: Suppose a company has 5 partnerships, each yields maybe 2 more deals a year via our tool, each deal $50k => $500k additional revenue. Even a 10% contribution margin means $50k net, so paying $10-20k a year is no-brainer. That’s how we justify pricing.

Add-On Services and Upgrades

We might have add-ons:
	•	Additional partner slots (if mid-tier has a limit).
	•	Additional data (if they want to sync huge data beyond tier).
	•	Integration add-ons: e.g. if they want a custom integration not standard, we charge a setup fee.
	•	Perhaps a “Partner Ecosystem Graph Analytics” module in future (like advanced network analysis) as upsell.

We should consider a usage-based element in case some customers are small but have heavy usage or vice versa:
	•	e.g. If a customer has 50 partners they actively co-sell with (like a hub company), maybe base price + extra per 10 partners beyond included.
	•	Or if they sync 1M accounts, maybe an extra charge for the heavy data.
However, too complex pricing could deter. We might keep initial pricing simple, then later incorporate usage metrics as we understand them (maybe quietly monitor usage to ensure we’re not drastically undercharging some heavy user).

Packaging and Feature Differentiation
	•	Free vs Paid features: Free gets basic mapping and Slack/email notifications. Automation rules likely a paid feature (that’s our key differentiator, we want paying users on it). Free might get 1-2 preset automations (like always email on new overlap) to tease the capability.
	•	Support level: Free – community/self-service support only. Pro – email support, maybe a shared Slack channel with our success team if needed. Enterprise – dedicated CSM, faster SLA support, maybe on-site workshop or integration help.
	•	Security/Compliance: Enterprise might include signing a BAA or security rider, custom data retention, etc., which is overhead for us, hence higher cost.

Network Considerations

A unique aspect: our product value increases as more companies join.
We should avoid pricing that penalizes inviting partners. Possibly allow inviting any partner (including free ones) at no direct cost, because each connection yields value that might lead them to convert too.

We might consider a “partner premium” where if both organizations are paying customers, they unlock extra features in their collaboration (for example, enhanced data sharing or multi-way workflows). But that might complicate things and harm free users’ experience with paying partners. Generally, we want paying customers to be able to collaborate with non-paying ones seamlessly (like Crossbeam does).
We’ll monetize primarily per org usage, not requiring both sides to pay.

Discounting & Trials
	•	We’ll offer a free plan rather than a limited trial, which covers the trial use-case.
	•	For Pro/Enterprise, we can do an initial free pilot (like 30-day full feature trial on request for serious prospects, possibly bridging to pilot plan).
	•	Annual contracts likely will be standard for enterprise deals (with some discount vs monthly).
	•	Possibly non-profit or startup discounts if any strategic reason (not focus now).

Example Pricing Structure (to illustrate):
	•	Free: 1 user, 2 partners, 500 accounts synced, Slack notifications (basic), no automations. $0.
	•	Pro: Up to 10 users (additional users $50/mo each), up to 10 partners (additional partners $100/mo each), up to 100k accounts, full feature set (automation, all integrations). ~$1,000/mo (paid annually $12k).
	•	Enterprise: Unlimited users and partners, >100k accounts, advanced features, priority support. Starting $3,000/mo (custom depending on scale) – can negotiate to $50k+ annual deals for very large.

We must remain flexible to adjust as we get market feedback. The key is to ensure the price correlates with value delivered and size of customer:
	•	A startup with 1 partnership should be free or very cheap (we want them onboard to grow).
	•	A scale-up with 10 partnerships deriving $500k pipeline via us should pay moderate (tens of k$).
	•	A large tech giant with 50 partners and huge data reliant on us could pay significant six figures because we’re critical infrastructure (similar to how some pay Crossbeam a lot for their scale).

Unit Economics & ROI Model

We will produce a ROI calculator for prospects:
For example, input number of partner-sourced deals per quarter, avg deal size, etc. Show how with our platform they might increase number of deals by e.g. 20% (due to uncovering hidden opps and accelerating deals) ￼ ￼. Then output incremental revenue. Compare to cost.

From Cloud Ratings, Crossbeam has ~5 months payback ￼. We aim for similar or better. So if our annual cost is $12k, we want to show we can generate >$24k in pipeline quickly (which is usually one small deal). That’s typically easy to justify if the tool is used.

We’ll also highlight intangible ROI:
	•	Time saved (no more spreadsheet matching calls which took hours per week) – that is productivity gain. E.g. account mapping can increase deal closure rate up to 70% and save countless hours ￼, which we can cite in marketing.
	•	Enhanced partner relationships (maybe measured by partner satisfaction but that’s softer).

Market Comparisons

We’ll position our pricing below Crossbeam at the mid-market level to entice switching or choosing us:
Crossbeam’s entry paid is $150/user/mo (~$450/mo for 3 users minimum annually) and then jumps to big plan $4.8k/mo for unlimited ￼. We could have a plan in between or more granular.

We might adopt per-user pricing publicly for simplicity (since many SaaS buyers understand that):
Say $100/user/month billed annually, min 5 users for Pro. That’s $500/mo ($6k/year) – quite affordable, possibly too low for what we deliver though. And if a company has 10 users that’s $12k/year, still comfortable for mid-market budgets.
But tying to user count might limit revenue for large ecosystems where maybe only 5 partner managers but they manage 50 partners and huge deals. We’d be undercharging them if just per seat. That’s where usage-based (per partner or per account volume) can capture more value.

Maybe we present it as modules:
	•	Base platform (for X users, Y partners) and expansion packs for more partners, etc.

For initial go-to-market, we might keep pricing simpler and low to get reference customers. Over time, as product proves ROI, we can raise or restructure.

Conclusion on Pricing

We will:
	•	Offer a Free tier to drive adoption and viral partner invites.
	•	Monetize via Pro (mid-market) tier at competitive rates (less than similar usage on Crossbeam/Reveal) to remove price objections.
	•	Use Enterprise custom deals to capture large accounts’ willingness to pay and cover extra services.
	•	Emphasize ROI: Partners who invest see returns quickly (we’ll gather case studies to show e.g. “Using our platform, ABC Co. generated $250k in partner-sourced pipeline in 3 months, for an investment of only $5k – a 50x ROI”).

We will also remain nimble; pricing can be adjusted after gauging market response in pilots. Initially, acquiring users is priority, so we might even onboard first few enterprise design partners for free or at cost in exchange for case studies and feedback, then solidify pricing once we have proof points.

Execution Plan

To execute this project, we’ll follow a 90-day roadmap for the MVP and an extended 6-month plan for further development. We’ll also outline team and resourcing, and key risks with mitigation.

0–30 Days (Month 1) – Foundations & Design

Goals: Establish core team, clarify requirements, design architecture, and make initial technical decisions. Begin development on the most critical pieces (data model and basic account mapping functionality).
	•	Team Assembly & Environment Setup – (Deliverable: Dev environment ready, team roles assigned). Form the founding team: likely 1 tech lead (Django expert), 1 full-stack engineer, 1 front-end (HTMX/Tailwind) developer, 1 product manager (could be one of founders doubling role), plus a part-time designer if possible for UI polish. Set up code repository, CI pipeline, and development environment (likely Docker for local Postgres, etc.). Define coding conventions and branching strategy for rapid collaboration.
	•	Requirement Refinement & User Research – (Deliverable: PRD v1.0 finalized). The team will refine the PRD by conducting a few user interviews with target partner managers to validate features and priorities. We will also review competitor products more hands-on (if possible, sign up for Crossbeam free or watch demo videos) to ensure we at least meet the baseline functionality. Finalize the exact MVP scope and acceptance criteria based on this (adjust if needed).
	•	Data Model & Schema – (Deliverable: Database schema implemented, migrations ready). Using the models outlined, implement the Django models: Organization, User, Account, Partnership, Overlap, etc. DB Tables touched: all core tables created. We will run initial migrations and test with sample data. Ensure the schema supports multi-tenancy (org foreign keys in place). Create some fixtures or seed script with dummy orgs and accounts to validate relationships. KPI: Not directly user-facing yet, but internal milestone – ensure the schema can handle expected volume and queries (maybe create 1000 dummy accounts and test a join).
	•	Also set up indexes on key fields (domain, org relations) from the start, to avoid forgetting.
	•	Core Platform Backend – Start coding the fundamental logic:
	•	Account Import Service: Develop a stub for importing accounts from a CSV (as initial easiest path before tackling API integrations fully). (Features & DB): touches Account table. UI: a simple form to upload CSV. This will let us test end-to-end mapping early using dummy data. Acceptance: can ingest a file and populate accounts for an org.
	•	Matching Algorithm (baseline): Implement the deterministic match function (domain match). DB: uses Account table data; populates Overlap table. Possibly implement as a Django management command or function now. No UI yet beyond possibly an admin view to trigger it. But we want to test that given two sets of accounts, we get correct overlaps in the DB.
	•	User & Org management: Basic signup/login flows using Django auth. Possibly integrate an invite code or allow signups freely at first. UI screens affected: Signup page, basic Org profile page. This is straightforward with Django allauth or similar.
	•	Setup initial permissions: e.g. use Django’s request.user.org to filter Account queries. Not heavy yet, but keep in mind.
	•	Front-end Framework and Design – (Deliverable: UI prototype for key screens). Create initial Tailwind CSS design system (colors, spacing, basic components for buttons, tables, forms). Design wireframes for key screens: Overlap list, Partner list, Intro request modal, etc. We might use a tool like Figma for quick mockups then implement directly in code. Focus on one primary flow: after login -> connect data -> view overlap. Use placeholder data to design these pages.
	•	Possibly implement a simple navigation layout (sidebar or topbar with menu for Partners, Overlaps, Settings, etc.). UI screens to start coding: Dashboard (empty state), Invite Partner page.
	•	Slack/Chrome strategy (Plan) – In this phase, we likely won’t implement Slack or extension yet, but we outline how we’ll do it so that when coding core, we leave hooks for notifications. For example, decide to use Slack webhooks vs full app, etc. Possibly register a Slack app name to reserve it.
	•	Review and Iterate – End of month, do an internal demo using dummy data flowing: e.g. simulate two orgs, upload CSVs, run overlap logic, show overlaps in a basic HTML table. This checks the backbone. Adjust any modeling issues discovered (e.g. need to tweak field lengths or add an index). Confirm the flow makes sense.

By Day 30, we aim to have a “steel thread” through the system: the simplest end-to-end scenario working in development:
A user can create an account, create Org, input some account data (via CSV), invite a partner (maybe simulate acceptance by having multiple logins), and see an overlap computed. The UI will be very rough, but functional for that loop.

31–60 Days (Month 2) – MVP Feature Implementation

Goals: Complete all MVP functionalities identified, integrate with at least one CRM (HubSpot likely for ease), and begin testing with a friendly pilot user by end of this phase.
	•	CRM Integration (HubSpot) – (Deliverable: Working HubSpot data sync). Build the first real integration:
	•	Register a developer app in HubSpot, get client ID/secret.
	•	Implement OAuth flow (a Django view that handles callback). UI: in Org Settings, “Connect HubSpot” button.
	•	On success, store token in IntegrationCredentials. Kick off initial data fetch from HubSpot’s companies API.
	•	DB Tables: Account (populating it). Background job: likely set up Celery now to perform the sync outside request cycle.
	•	Once data is in, trigger the overlap calculation for any existing partnerships (maybe none yet if partner not connected). But we can test by connecting two orgs both with HubSpot data.
	•	Acceptance: User can connect HubSpot and see their accounts listed in an “Accounts” page (for debugging perhaps), and if partner also connected, overlaps appear. KPI: number of accounts imported, time to import. We’ll set a modest target e.g. handle 5k accounts in <5 minutes for now.
	•	Partnership Invitation Workflow – (Deliverable: Partner onboarding flow working).
	•	Implement sending an invite email with a signup link (use Django Email). Could use a simple template.
	•	Create a view for accepting invites: if a user signs up via that link, tie them to the invited partner org record.
	•	UI: On one side, show pending invites; on the invitee side, show a welcome page guiding to connect data.
	•	Ensure once both are active, the partnership status flips to Active and triggers an initial overlap calculation.
	•	DB: Partnership, User. Possibly add a field in Partnership for invite_token to embed in link.
	•	Acceptance: A can invite B (maybe test by using two different email addresses), B signs up and connects data, A sees that B joined. Overlaps (if any data) become visible.
	•	Overlap Listing UI & Intro Request – (Deliverable: End-to-end co-sell workflow in UI).
	•	Create the Overlaps page: For a selected partner (choose from a dropdown or partner list page), display overlaps table. Use HTMX to load this asynchronously if needed.
	•	Show relevant fields: e.g. Account Name (mine), Account Name (partner) – though partner’s account name we might anonymize until certain trust? But presumably if they connected, they agreed to share name at least. We’ll show it, as crossbeam does.
	•	Implement basic filtering (maybe just a search box for account name).
	•	Intro Request: On each overlap row, add “Request Intro” button which opens a modal (HTMX could fetch a form fragment). Form has message field prefilled (“Hi, can you intro us to Acme…”). Submit triggers a Django view: it creates an IntroRequest record (or sets Overlap.status = Requested, and maybe store message).
	•	Notify partner: Mark something so that partner sees it next time they refresh overlaps page. For realtime, maybe send an email notification to partner’s partner manager: “You have a new introduction request from Org A for Account X.” (Slack later, but email for now).
	•	Also reflect in UI: requestor sees the overlap row now says “Intro Requested on DATE”.
	•	Implement Partner’s view to Accept/Decline: e.g. an “Incoming Requests” page or highlight on overlaps list (could filter overlaps by status). On Accept, a form to optionally provide contact info or note. On submission, mark Overlap.status = Accepted, store note.
	•	Email notify the requester on acceptance (with the shared contact info perhaps).
	•	DB Tables: Overlap and/or a separate IntroRequest table if we made it. If just Overlap, add fields for requested_by, requested_at, accepted_at, etc.
	•	Acceptance criteria for feature: Two simulated users (one each org) can go through invite -> request intro -> accept, and both see the correct updated statuses in their UI. Data integrity: The partner B should only see requests where they are target, etc.
	•	KPI impact: Track count of requests made in test, ensure states flow. Not user KPI yet, but this is a core usage metric later.
	•	Automation (Basic) – (Deliverable: Basic notification automation functioning).
	•	Implement at least one automatic notification as a proof-of-concept of the Execution OS. For MVP likely:
	•	On New Overlap: send an email to the account owner (if we have owner email from CRM) or to a default user.
	•	Or On Intro Accepted: email the AE with “Your partner intro was accepted, here’s contact”.
	•	This can be hardcoded logic initially inside the respective view or model save triggers. For example, after saving Overlap, if conditions match, call email function.
	•	Use Django’s email backend (could be console or SMTP to simulate).
	•	If time allows, set up Slack simple message: If we have Slack token, possibly send a message via requests.post to Slack webhook (for now maybe skip fully real Slack until month 3).
	•	This step is to validate our approach to automation and gather feedback on usefulness.
	•	KPI: We’ll measure if events trigger correct actions (in test environment). Not externally measured yet.
	•	Slack Integration (Basic) – (Deliverable: Slack notification via app or webhook).
	•	Register a Slack app (we can use incoming webhook for simplicity at first: Slack allows creating a webhook URL for a channel). For MVP, even just have an environment variable for Slack webhook of a test workspace channel.
	•	When sending a notification (like new overlap), post to Slack channel. This will show the team how it would work.
	•	Not building full Slack app with OAuth yet in MVP (maybe push to V1 unless easy).
	•	Confirm message appears in Slack test channel, containing the relevant info. This will excite stakeholders that integration is working.
	•	Chrome Extension (Design) – Probably no time to implement by day 60; but we can design how it will work. If small team, this might be deferred to after MVP. But at least speccing it out is done. We focus on core web app first.
	•	Testing & QA – As components get built, we write unit tests (especially for matching logic, to ensure e.g. “Inc.” vs “Inc” matches correctly). Also test integration flows (simulate token expiration, etc).
	•	Start internal QA checklist: test inviting existing user (should handle gracefully?), test invalid data handling, etc.
	•	Possibly set up a staging environment to deploy and test as if production.
	•	Pilot Preparation: Around day 50-60, identify a potential pilot customer (maybe a friendly company or even one of our companies if internal). Plan data import for them. We might start with ourselves: e.g. load our partnership data or a known example to ensure ready for an external user.

By Day 60, we should have a functionally complete MVP:
A user can onboard, integrate HubSpot, invite partner, see overlaps, request intro, get notified, and track outcome. It may still be rough on UX (some polishing needed), and not all edge cases handled, but core is there.

We’ll likely demonstrate this to a pilot user or advisor at end of Month 2 to get feedback.

61–90 Days (Month 3) – Pilot Launch & Refinement

Goals: Onboard first pilot users, validate the product in real-world conditions, and iterate on any issues or missing pieces. Begin work on scaling and additional integration (Salesforce likely in this phase). Also, polish UI/UX and address any critical gaps.
	•	Pilot Onboarding (Day ~65-70): Bring on 1-2 pilot partner teams:
	•	Possibly ask each to invite one of their partners so we have at least one real partnership on the platform.
	•	Assist them in connecting their CRM. If they use Salesforce, this might force us to expedite Salesforce integration now if not done yet.
	•	Gather their data and manually ensure matching is correct, adjust matching rules if needed.
	•	Provide a short training and be on standby as they try an intro request etc.
	•	Success metric: Within first 1-2 weeks, each pilot should have identified at least a few valuable overlaps and initiated some co-sell action. We’ll define 30/60/90 day success metrics below, but by day 90 (end of pilot phase), we hope to see tangible pipeline or meetings from it.
	•	Salesforce Integration: If any pilot user uses Salesforce (likely yes), implement that now:
	•	Build OAuth connection for SF similar to HubSpot. Possibly use a library or direct API.
	•	Test syncing a subset first (maybe only accounts where IsPartner or something to reduce volume initially).
	•	By end of month 3, at least one Salesforce org should be syncing.
	•	This is critical for going beyond pilot as many companies will need it. So we aim to complete core SF integration by day ~80.
	•	Scalability & Performance Tuning: With real data from pilots:
	•	If one loaded 10k accounts, observe overlap calculation time. Optimize queries if slow (maybe add missing index, or adjust algorithm).
	•	Test concurrent usage: maybe simulate two partners hitting at same time or multiple users.
	•	Ensure the background jobs are running smoothly (no memory leaks, tasks not stuck).
	•	Possibly implement partitioning if needed (likely not yet).
	•	Performance target by day 90: Overlap query or page load < 2 sec for pilot-scale data; sync jobs finishing in reasonable time (within an hour for initial, within minutes for incremental).
	•	UX Polish & Bug Fixing: Based on pilot feedback:
	•	Improve any confusing UI text or flows. E.g. if they found inviting partner unclear, add tooltips or a better invite email template that explains value.
	•	Fix any UI bugs (layout issues, modals not closing properly, etc.).
	•	Make the app look more professional: apply consistent Tailwind styles, our branding (logo etc).
	•	Add a basic dashboard home that perhaps shows summary (like “You have 5 overlaps needing action”) to guide user to next steps.
	•	Security & Compliance Preparations: If pilots include larger companies, they may ask about security:
	•	Implement at least basic measures: enforce HTTPS, secure cookies, multi-factor auth for admin accounts perhaps.
	•	Draft a simple privacy policy and get NDA if needed with pilot participants.
	•	Not full SOC2 yet obviously, but ensure we log access and can trace any issue.
	•	Monitoring & Analytics Setup: Set up internal logging:
	•	Use a tool (like Sentry) for error tracking in the app.
	•	Basic usage metrics capture: instrument events like “IntroRequested”, “OverlapViewed” perhaps to our analytics (could be in DB or a service like Mixpanel for early stage).
	•	These will help measure pilot engagement (DAUs, actions per user).
	•	Prepare Documentation: Write a simple user guide for pilot users (how to connect, how to interpret overlaps, best practices for co-selling). Also internal docs for support.
	•	Metrics to track during pilot:
	•	Number of overlaps found per partnership.
	•	Number of intro requests made, and their outcomes.
	•	Time from overlap appearing to intro request (indicates if notifications are prompting quick action).
	•	Feedback from users (qualitative).
	•	We’ll gather these to measure against our success criteria (e.g. target: at least 5 intro requests in first month from pilot co-selling, etc.).

By Day 90, we intend to have:
	•	2-3 real partnership pairs actively using the platform.
	•	At least one success story (even if small, like “We secured a meeting at Account X through the platform that we wouldn’t have known about otherwise”).
	•	MVP refined to the point it’s stable and user-friendly enough for broader beta invite.

6-Month Roadmap (Beyond 90 days to ~180 days)

From month 4 to 6, we focus on MVP to V1 enhancements and scaling market launch:
	•	Months 4-5: Incorporate wedge differentiation features:
	•	Build out the Automation Rule Builder UI for Execution OS vision (as per product PRD V1).
	•	Enhance matching with fuzzy logic (maybe in month 5).
	•	Add the Chrome Extension development (month 5 likely), so by end of month 6 we can demo it.
	•	Integrate additional services: Possibly Microsoft Dynamics integration if needed for some prospects, or at least plan it.
	•	More Slack integration (full OAuth and slash commands).
	•	Hardening the system for more users: improvements to permission granularity, implementing SSO for enterprise.
	•	Month 6: Official Beta Launch with more customers:
	•	Onboard 5-10 customers beyond pilots, potentially paying (maybe offer a promotional pricing).
	•	Work on sales & marketing collateral (case study from pilot, ROI calculator, website).
	•	Implement any needed features that came as pilot feedback (e.g. “We need multi-partner view” or “We want a CSV export of overlaps”).
	•	Technical: scale infrastructure (maybe move to cloud auto-scaling, ensure backups and DR in place).
	•	Staffing plan for 6 months: Initially ~3 devs, 1 PM, maybe a part-time UX designer. As we approach bigger rollout, consider adding:
	•	1 more backend engineer for integration heavy-lifting.
	•	1 devops part-time or consultant to optimize deployment, security (especially for enterprise readiness).
	•	A customer success/solutions person by month 6 to handle pilots and onboarding, ensuring they succeed (co-founder PM might fill this role early, but will need help as users grow).
	•	Possibly a data analyst or ML engineer if fuzzy matching or analytics demands.
	•	Keep team lean initially, then scale up as more customers sign (reinvest revenue into team expansion).
	•	Risks at this stage (and continuous mitigation):
	•	Data security risk: We will engage a security review by month 6 (maybe hire a consultant to do a penetration test) because partner data is sensitive. Mitigation: early implementation of best practices, encryption, and obtaining at least SOC2 in year 2.
	•	Scalability risk: If an unexpectedly large customer onboards (with huge data) early, could strain system. Mitigation: ensure architecture can quickly adapt (maybe keep an eye on heavy DB queries, implement caching if needed).
	•	Adoption risk: Partners might be reluctant to adopt a new platform (the “Cold start problem” – they might say “just email me the overlap spreadsheet”). Mitigation: invest in change management, make the product extremely easy and beneficial (the free tier strategy, plus building features like Slack integration so it meets them where they work). Also possibly incorporate incentives (like some gamification or tangible benefit to partner reps responding quickly).
	•	Competitive response: Crossbeam or others might announce new features or drop prices seeing new entrants. Mitigation: Focus on our niche (automation) and agility. Also possibly avoid direct head-to-head initially; instead capture customers that are underserved (like smaller ones or those wanting more action).
	•	Legal/Compliance: We deal with customer data. Risk of accidentally exposing something or compliance with GDPR (if EU data). By 6 months, have legal counsel review our data policies, start prepping for GDPR compliance steps (like data processing addendums, etc).

Risk Register & Mitigations (Summary)

Risk	Likelihood	Impact	Mitigation Strategy
Data breach or privacy leak	Low	Critical	Implement strict access controls, encryption. Conduct security testing. Limit data shared to only overlaps ￼. Get NDA/agreements with pilot users to build trust.
Low adoption / partner invites not accepted	Medium	High	Make onboarding dead-simple (no heavy IT work). Emphasize mutual benefit to partners (both win). Use free tier to eliminate cost barrier. Provide resources to help champion internally (ROI, best practices). Possibly directly integrate into tools (Slack, CRM) to reduce friction.
Matching inaccuracies leading to lost trust	Medium	High	Start with conservative deterministic matches (high precision) ￼. Validate with users during pilot. Add user ability to correct matches. Rapidly improve fuzzy logic based on feedback. Communicate clearly when a match is “suggested” vs confirmed.
Feature creep delaying MVP	Medium	High	Stick to core use case for MVP. Use feedback to prioritize must-haves vs nice-to-haves. The wedge hypothesis guides us to focus (Execution OS features first, etc.). Maintain an explicit scope document and push non-essentials to backlog (e.g. multi-partner analytics to V2).
API integration limits or changes (Salesforce, etc.)	Low	Medium	Build integrations using official APIs and follow best practices. Monitor for API deprecations. Have contingency to increase API quotas if needed (work with customer to get higher tier API access if heavy usage). Keep the integration layer abstract so we can swap approach if needed (e.g. if hitting limits, we could use a middleware like an ETL tool).
Competition reaction (Crossbeam adding automation)	Medium	Medium	Continue to differentiate by speed and focus. Even if they add similar features, our smaller size can mean more personalized support and faster iterations. Also, target segments they might not focus on (mid-market self-serve). Build switching incentives (easy import from Crossbeam, etc.).
Talent/Resource risk (small team)	Medium	Medium	All key knowledge shared among team to avoid single point failure. If needing expertise (e.g. devops or ML), plan to contract or hire promptly. Keep morale and vision strong to retain early team.
Regulatory compliance (GDPR, etc.)	Low	Medium	Early on, implement basics: consent for data sharing between partners built into platform usage terms. Provide ability to delete data if requested. Long term, plan for compliance certifications as needed when targeting enterprise in EU.

We will review and update this risk register regularly and ensure owners for each risk mitigation (e.g. CTO for security, PM for adoption, etc.).

Epics & Deliverables Breakdown (Summary)
	•	Epic 1: Core Data & Matching Engine – Deliverables: Org & Account models; CSV import; Deterministic matching function; Overlap DB and query. (DB: Account, Overlap; UI: none or basic admin; Integration: CSV or dummy; KPI: readiness of match algorithm measured by internal tests of precision)*.
	•	Epic 2: User Onboarding & Partner Connections – Deliverables: Signup/login flows; Invite partner via email; Accept invite; Basic sharing settings. (DB: User, Organization, Partnership; UI: Signup pages, Invite screens; Integration: email SMTP; KPI: time to connect a partner, invites accepted)*.
	•	Epic 3: CRM Integration – Deliverables: HubSpot connector (OAuth + import job); (later Salesforce connector); Data refresh schedule. (DB: IntegrationCredentials, populate Account; UI: Connect CRM button & status; Integration: HubSpot API; KPI: accounts synced count, sync latency)*.
	•	Epic 4: Overlaps UI & Workflow – Deliverables: Overlaps listing page with filters; Intro Request submission & tracking; Accept/decline flow. (DB: Overlap status fields or IntroRequest; UI: Overlap list, Request modal, Notification indicators; Integration: email for notifications; KPI: # overlaps viewed per user, # requests sent/accepted)*.
	•	Epic 5: Automation & Notifications – Deliverables: Basic event triggers (new overlap alert, intro response alert); Slack notification setup; Email templates; Rule engine schema (prep for more rules). (DB: maybe AutomationRule for design; UI: minimal for now; Integration: Slack webhook, SMTP; KPI: average notification lag, user response time to events)*.
	•	Epic 6: Integration Expansion – Deliverables: Salesforce integration fully working; Slack slash command; Chrome extension beta. (DB: account linking SF IDs; UI: for Slack maybe none, extension has its own; Integration: Salesforce API, Slack API, Chrome runtime; KPI: not for MVP, but usage of these in pilot)*.
	•	Epic 7: Analytics & Reporting – Deliverables: Basic dashboard metrics (counts, charts maybe using a JS library or simple tables); possibly an export CSV function for overlaps. (DB: might use Overlap or events; UI: Dashboard page; Integration: n/a; KPI: not external, but helps demonstrate value in pilot)*.
	•	Epic 8: Polishing & Hardening – Deliverables: UX improvements (loading states, error messages); Security features (CSRF, permissions thoroughly enforced); Performance optimization (index tuning, query caching if needed). (DB: maybe add indexes/partitions; UI: error modals, spinners; Integration: n/a; KPI: page load times, pilot user satisfaction survey)*.

Each epic involves multiple features and we enumerated the (a) DB, (b) UI, (c) integration, (d) KPI where applicable above.

We’ll track epics in a project management tool (Jira/Trello) and have deliverable demos at end of each to ensure we’re on track.

Demo Script & Pilot Validation Plan

Finally, we prepare how to demonstrate the product and define success metrics for pilots:

Demo Script (for a prospective user or stakeholder)

Setup: For demo, we assume two companies: AlphaCorp (our company) and BetaInc (partner). AlphaCorp uses our platform and has invited BetaInc. We have pre-loaded sample CRM data: AlphaCorp’s pipeline and BetaInc’s customer list.

Step 1: Introduction (Main Title Slide) – “Meet CoSellOS: Turning Partner Signals into Revenue.” (Show a one-liner of what it does). Then go into the live demo:
	1.	Account Mapping Discovery: On screen: AlphaCorp’s Partner Dashboard. “Here I’m a Partner Manager at AlphaCorp. I’ve connected our CRM and invited my partner, BetaInc.”
	•	Navigate to “Partners” section where BetaInc is listed as Connected. Point out data was synced automatically from CRM ￼.
	•	Click BetaInc to view overlaps. Show Overlap Table: “The platform has automatically identified X overlapping accounts between AlphaCorp and BetaInc ￼. We can see, for example, Acme Corporation appears – on my side it’s a target prospect (no current business), and BetaInc shows it as a current customer ￼. That’s a golden co-sell opportunity.”
	•	Highlight that only overlapping accounts are visible, preserving privacy of non-overlaps ￼.
	2.	Requesting an Introduction:
	•	“Instead of sending emails or spreadsheets, I can take action right here.” Click on Acme Corporation row, hit “Request Intro”.
	•	Show Modal: A pre-filled introduction request message appears. “I can customize this message if needed. It already indicates I’d like an intro to Acme Corp’s decision-maker.” Press send.
	•	UI updates: The Acme row now is flagged as “Intro Requested – pending response”.
	•	Emphasize: “This saves me time – BetaInc’s team is instantly notified, and it’s all tracked here.”
	3.	Partner’s Perspective:
	•	Switch browser or screen to BetaInc’s view (we’ve logged in as BetaInc’s partner manager on another account).
	•	Show Notification: “BetaInc’s partner manager gets a notification in Slack immediately ￼ – here I see in our Slack channel: AlphaCorp requested an intro to Acme Corp.” (If Slack demo possible, show the Slack message with interactive link).
	•	In the BetaInc app view, go to “Requests” or overlaps: see Acme highlighted with a request. Click “Review Request”.
	•	Show Accept Screen: BetaInc sees the context (Acme is their customer, with maybe key contacts listed from CRM). They click “Accept & Introduce”.
	•	“BetaInc can type a note – e.g. ‘Happy to introduce you to our client’s VP, I’ve CC’d you on an email.’ – and hit Accept.”
	4.	Closing the Loop:
	•	Switch back to AlphaCorp view. Show that the Acme Corp row updated: “Now I see BetaInc accepted my intro request, and even provided the contact info (see the note). I also got an email notification with those details.”
	•	Mention: “Our Salesforce (or HubSpot) CRM was also automatically updated – let’s check AlphaCorp’s Salesforce.” (Pull up Salesforce account page for Acme – show a custom field or note ‘Partner Intro in progress’ that our platform wrote ￼).
	•	“So my sales rep will see right in CRM that BetaInc is helping with an intro, no extra manual data entry.”
	5.	Automation Example:
	•	“Additionally, our platform automatically creates tasks and reminders. For instance, if BetaInc hadn’t responded in 3 days, it would’ve sent them a reminder email (or Slack ping) so I don’t have to chase them manually – the system does it ￼.”
	•	Open the “Automation Rules” settings page: show a sample rule: “If no response to intro in 3 days, notify partner” (just for illustration).
	•	Maybe toggle another rule: “Auto-notify account owner on new overlap” – explain how a sales rep would get notified when a partner has a lead on their target.
	6.	Dashboard & Metrics:
	•	“All these partner interactions are tracked. Let’s look at the Dashboard.”
	•	Show Dashboard: “In the last 30 days, we’ve identified 25 overlaps with BetaInc, initiated 5 intros, and already 2 deals are now in progress due to those intros – as seen here with an estimated $XYZ pipeline ￼. Partner-sourced revenue is now a measurable number for us.”
	•	Point out a specific metric: e.g. “Partner-sourced Pipeline: $200k”, “Deal win rate on partner-introduced opps: 30% higher than average” if we have that (could simulate).
	7.	Chrome Extension (if ready):
	•	“Before we wrap, one more neat thing: our Chrome Extension. Say I’m a sales rep on LinkedIn researching a prospect…”
	•	Switch to a LinkedIn page for Acme Corp. Click our extension icon.
	•	Popup shows: “Acme Corp – Partner overlap found: BetaInc is a customer -> Consider reaching out to BetaInc for intro.”
	•	“Right there in LinkedIn, I get the insight without even logging into any app.”
	8.	Conclusion of Demo:
	•	Summarize: “In a few minutes, we identified a hidden opportunity, collaborated with our partner frictionlessly, and moved the needle on a deal – all inside this platform. Imagine doing this at scale with all your partners – it’s a game changer for ecosystem-led growth ￼.”
	•	“No more guessing which partners have leads or endless emails – CoSellOS gives you real-time partner data and the tools to act on it instantly ￼.”

Throughout the demo, we cite how each step ties to solving common pain:
	•	Manual spreadsheets replaced by live data ￼.
	•	Missed opportunities now surfaced (mention that study: account mapping can increase sales by 30% or such ￼).
	•	Slow partner response improved by automation (reminders).
	•	And the synergy: pipeline attributed to partners ($ value).

We also mention security briefly: “Note, BetaInc never saw our full list, only overlaps, and vice versa – trust is built-in ￼. We comply with privacy – e.g. data is encrypted and only shared with consent.”

The demo script covers the main user journey and highlights our wedge (automation notifications and integrated actions).

Pilot Validation Plan (30/60/90 Day Metrics)

For each pilot (let’s say pilot includes 2 companies partnering):
	•	Day 0 (Start): Set baseline metrics:
	•	Number of known overlaps from their current methods (if any).
	•	Current partner-sourced pipeline or deals (if trackable, likely low or anecdotal).
	•	The pain points they have (qualitative baseline, e.g. “takes 2 weeks to get an intro”).
	•	Train users on platform usage.
	•	30 Days:
	•	Activation & Engagement Metrics:
	•	Target: 100% of pilot users onboarded and using the platform. Specifically, each partnership should have at least one data sync completed (CRM connected) and one overlap review meeting in first week.
	•	By day 30, expect that each pilot user logs in at least weekly (we measure WAU).
	•	Each pilot partnership should have identified >=5 overlaps (depending on data; if their overlap is small naturally, then whatever is found).
	•	Intro Requests: At least 2 intro requests sent in first month by each partner manager.
	•	Response Time: Measure time from request to partner response – goal is improvement over their prior baseline (if previously took 1-2 weeks via email, now hopefully a few days or less).
	•	Feedback: Collect qualitative feedback at 2-week mark: is the system easy? any confusion? Use this to fix UI friction quickly.
	•	60 Days:
	•	Pipeline Metrics: By two months, some intro requests should have progressed to actual sales opportunities.
	•	Track # of partner-influenced opportunities created. Goal maybe 1-3 new opps per partnership by 60 days (depending on their sales cycle).
	•	Track progress: e.g. “of 4 intro requests, 3 resulted in meetings, 2 progressed to proposals”.
	•	Usage metrics: Both sides of each partnership should still be engaged. Check if any user has dropped off. Aim for retention: e.g. >=80% of invited users are still active by day 60 (some drop maybe if a rep saw but not daily user).
	•	Automation usage: Ensure at least one or two rules are in use. e.g. pilot user set up a custom rule or is benefiting from default ones (like Slack alerts). We might survey “Has the Slack notification feature been helpful?” expecting yes.
	•	Satisfaction: Survey pilot users (scale 1-10) on ease of use and perceived value. Aim for high (>=8) by day 60 if possible (if lower, identify why and address quickly before day 90).
	•	Address any lingering technical issues (maybe do a minor version update around day 60 based on first two months feedback).
	•	90 Days:
	•	Outcome Metrics: Hopefully by ~3 months, one of those partner-introduced opportunities has closed or is very close to closing. If sales cycles are long, at least have quantifiable pipeline:
	•	e.g. “$100k of pipeline sourced via partners in 3 months” – which we compare to baseline (likely baseline was near $0 purely partner-sourced).
	•	If possible, a closed-win deal attributable to the co-sell process. Even if a small upsell or quick win, that’s gold for case study.
	•	Efficiency Metrics: Document reduction in time or effort:
	•	e.g. “We used to spend hours in quarterly business reviews swapping spreadsheets; now with CoSellOS we have data instantly – saving ~5 hours per partner per month” (we can gather anecdotal from them).
	•	“Our average partner intro cycle time (request to meeting) went from 2 weeks to 3 days thanks to the platform notifications.”
	•	Engagement: By 90 days, want this to be integrated into their routine:
	•	The partner managers should be logging in frequently (couple times a week at least). Ideally sales reps too are engaging (maybe via Slack extension or reading CRM notes).
	•	Check metric: maybe average of 10 overlaps viewed per week, or Slack notifications delivered.
	•	ROI: If any deal closed, calculate ROI: e.g. closed $50k deal, using product that (if priced) would cost maybe $5k for that period – that’s 10x ROI. If pipeline only, say $200k pipeline * 20% close rate = $40k expected value, still positive ROI.
	•	At 90 days, meet with pilot stakeholders to review these metrics:
	•	If success: get testimonial, ask to be reference, and discuss expanding usage (maybe adding more partners or users).
	•	If issues: identify what prevented bigger success – was it lack of partner responsiveness (non-product issue) or product features? Use that for next iteration.

Success Criteria for Pilot: Ideally:
	•	Both pilot partnerships continue using the platform voluntarily after 90 days (no dropout).
	•	They agree to convert to paying (or at least to roll into a next phase).
	•	We have concrete data: e.g. “In 3 months, [Pilot Company] discovered 15 new sales opportunities with partners, of which 5 are in active negotiation, representing $X pipeline – something they couldn’t do before.” And a quote like “It’s made co-selling so much easier and faster” to use in marketing.

We’ll also identify any metrics that didn’t improve:
	•	If a pilot had many overlaps but few intro requests, why? Possibly trust issue or aligning with sales team. We then might consider features to involve sales more (like notifying AEs more directly or gamifying intros).
	•	Or if lots of intros but no deals, maybe quality of overlaps – maybe refine matching or focus criteria (like highlight overlaps that are truly valuable).

We set 30/60/90 day checkpoints to adapt our approach:
	•	e.g. If by 30 days no intros sent, intervene: maybe they are hesitant, offer additional training or even manually facilitate one to get the ball rolling.
	•	Use that period to tune not just product, but also our customer success playbook for future clients (e.g. ensure early wins).

By carefully tracking these pilot metrics and adjusting, we ensure when we officially launch more broadly, we have evidence of success and a refined onboarding method that drives those early wins for every new customer, validating our wedge and product direction.

⸻

Throughout this comprehensive plan, we’ve detailed the competitive landscape, our strategic wedge focus (Execution OS), the product roadmap from MVP to advanced features, the technical architecture in Django/HAT, scaling considerations, matching algorithms, automation engine, integrations, pricing strategy, execution timelines, and validation metrics. With this plan, our founding team is ready to build an implementation-ready co-sell execution platform poised to deliver ecosystem-led growth results for our users.

Sources:
	•	Crossbeam overview and network scale ￼ ￼
	•	PartnerTap and Reveal positioning ￼ ￼
	•	Data showing need for automation beyond mapping ￼
	•	PartnerStack and PRM competitive insights ￼ ￼
	•	Data sharing and security requirements ￼
	•	Slack & Chrome extension integration benefits ￼ ￼
	•	ROI and partner co-selling impact stats ￼ ￼


	Here’s my straight take: this write-up is a strong narrative + decent stack direction, but it’s not yet an “executable plan” and it overstates the “execution vacuum” relative to what incumbents already ship today.

Below is a structured review: what’s solid, what’s wrong/hand-wavy, what’s missing, and how to upgrade it into research that can drive the wedge decision.

⸻

1) What this doc gets right (and is verifiable)

The Crossbeam ↔ Reveal consolidation is real, and “Supernode” is a fair framing
	•	Crossbeam publicly announced the merger and explicitly cites 25,000+ companies adopting Crossbeam or Reveal by 2024.  ￼
	•	Reveal’s standalone product/website has been decommissioned and users migrated to Crossbeam.  ￼

The “wedge / single-player utility” concept is the correct mental model

Network-effect products die in the “empty room” moment. A wedge that provides value before partner onboarding is strategically correct (even if the proposed wedges need sharper validation).

Django + HAT (HTMX/Alpine/Tailwind) is a plausible fit for the UI shape

For a table-heavy, filter-heavy, workflow-heavy product, server-rendered + HTMX can be faster to build and easier to keep performant than a large SPA—if you design paging, caching, and async jobs well.

⸻

2) Where it’s misleading or too confident

“Read-only dashboards” is overstated

Crossbeam explicitly markets (and ships) multiple execution surfaces:
	•	“Co-Selling Templates and Workflows” (including access via Copilot and Slack)  ￼
	•	“Slack and Alerts”  ￼
	•	“Deal Navigator”  ￼
	•	“Copilot” embedded in tools like Salesforce, HubSpot, Chrome  ￼

So: the “execution gap” might still exist, but it’s not “they don’t do execution.” It’s more likely:
	•	execution features are enterprise-gated, complex to operationalize, not adopted by reps, or don’t close the loop on outcomes + attribution.

The Chrome extension wedge is not clearly differentiated

This doc frames a Chrome extension as a Trojan horse, but Crossbeam Copilot already claims availability in Chrome (plus Salesforce/HubSpot/etc.).  ￼
So “Chrome overlay” alone is not a wedge. You’d need a different job: e.g., intro orchestration + rep opt-in + outcome capture + attribution writeback, not just “show overlaps.”

Pricing claims are loose

The write-up asserts “connector tax” and $10k+ integration gating. Crossbeam’s own pricing page emphasizes “Connector subscriptions” (annual, monthly terms with surcharge), but doesn’t support the specific $10k figure as written.  ￼
Third-party pricing summaries vary widely and should be labeled as estimates.  ￼

⸻

3) The biggest strategic gap: it doesn’t actually choose a wedge via evidence

It proposes wedges, but it doesn’t do the work you said you want: let research drive the decision.

What’s missing:
	•	Customer segmentation (who hurts most, who pays fastest)
	•	Sales cycle reality (partnerships teams vs RevOps vs sales leadership)
	•	Adoption friction model (rep opt-in, partner opt-in, permissions)
	•	Proof that incumbents fail on a specific workflow (with quotes + counts)

Also: it frames “mid-market” as the gap, but doesn’t show why mid-market can’t use PartnerTap / Crossbeam / PRMs (or what they hate most).

⸻

4) Technical critique (important)

(A) RLS multi-tenancy: good idea, but operational landmines

RLS with tenant_id + SET app.current_tenant can be excellent, but you must address:
	•	connection pooling (you must ensure the setting is reset per request; transaction pooling can break assumptions)
	•	background workers (Celery tasks need explicit tenant scoping)
	•	bypass risk (superuser DB roles, migrations, analytics jobs)

The doc treats it as “done” but doesn’t include the operational controls that make it real.

(B) “Double-blind hashing escrow” is hand-wavy

Hashing identifiers in a single platform DB is not automatically “double blind.” If you’re the processor, you can still join and infer. True “escrow” typically requires stronger protocols (or a trust/legal/security posture that substitutes for cryptography).

This matters because “secure account mapping” is the core trust promise.

(C) Materialized view + triggers for incremental overlap maintenance is risky

“Trigger recalculates overlaps for those 50 accounts” can turn into a write-amplification nightmare, create lock contention, and make ingestion unpredictable.

Most real systems do:
	•	incremental tables updated by background jobs,
	•	or batch recompute with partitioning/blocking,
	•	or a dedicated overlap index per tenant/partner_connection.

(D) Salesforce ingestion is directionally right but needs sharper specs

Using Bulk API 2.0 for initial load is correct.  ￼
But the doc’s “15,000 calls/day” example is not universally correct—limits vary by org edition and purchased capacity.  ￼

⸻

5) Evidence quality issues in the “works cited”

A lot of citations are blog aggregators and comparison sites. Those can inspire hypotheses, but they can’t carry core claims like:
	•	“users say UI is bloated”
	•	“essential features gated”
	•	“implementation time-to-value”
	•	“partners can’t operationalize overlaps”

To make research decision-grade, you want more of:
	•	vendor docs (Crossbeam/PartnerTap/WorkSpan/Workato-style docs)
	•	review platforms (G2/TrustRadius) with direct quotes + frequency
	•	buyer/operator podcasts / case studies
	•	integration docs (Salesforce/HubSpot/Slack) for real constraints

⸻

6) If you keep the “execution gap” thesis, here’s how to sharpen it into a real wedge

Because incumbents do ship execution features, your wedge needs to be one of these:

Wedge Type 1: “Close-the-loop execution” (the missing piece)

Make the product unbeatable at:
	•	intro request → intro acceptance → meeting booked → opportunity created/updated → outcome logged → attribution computed → CRM writeback
…and do it with rep-native UX (Slack/CRM), not partner-team dashboards.

PartnerTap explicitly markets co-sell automation + intros + alerts.  ￼
So the wedge becomes quality + speed + adoption, not existence.

Wedge Type 2: “Single-player value” that is actually urgent

The “internal data hygiene” wedge could work, but it’s vague. Make it specific and ROI-linked, for example:
	•	“Find accounts with open pipeline where partner customer success risk is high”
	•	“Detect channel conflict + duplicate outreach”
	•	“Auto-create ‘ecosystem coverage gaps’ list for ABM”
The wedge must produce value in < 7 days without partner opt-in.

Wedge Type 3: Vertical identity + compliance where generalists are weak

This is the most defensible wedge if you pick the right vertical and identifier system:
	•	healthcare (NPI, facility/location hierarchies)
	•	fintech (on-prem / self-host / region constraints)
	•	public sector / education (entity structures, procurement IDs)

This needs real validation interviews because “HIPAA mapping” sounds good but can be a sales-cycle trap.

⸻

7) How I would upgrade this into the “research-driven wedge decision” you asked for

If you want research to drive focus, your next iteration of this document must add three missing artifacts:

Artifact A — Competitor feature checklist (actual, not implied)

Build a matrix across Crossbeam / PartnerTap / WorkSpan / PRMs with rows like:
	•	Escrow model, offline partners, shared lists, rep workflows, intro automation, attribution, CRM writeback, Slack, Chrome, admin, audit, APIs, webhooks, SSO, etc.

Crossbeam’s public “key features” list is already a starting spine.  ￼

Artifact B — “Why pilots fail” dataset (quotes + counts)

Pull 50–100 review quotes and tag them into buckets:
	•	onboarding friction
	•	partner opt-in failure
	•	matching quality
	•	rep adoption failure
	•	attribution disputes
	•	CRM writeback pain
	•	pricing surprise
	•	permissions complexity

Then the wedge becomes: “we eliminate the top 2 failure modes.”

Artifact C — Unit economics + activation math

Even rough math forces clarity:
	•	cost to ingest + normalize + match per tenant
	•	storage cost for overlaps
	•	marginal cost per partner connection
	•	expected ACV by segment
	•	CAC and payback sensitivity

Without this, you can’t pick between “mid-market self-serve” vs “vertical compliance” vs “execution OS.”

⸻

Bottom line verdict
	•	Useful: narrative framing, stack alignment, the wedge concept.
	•	Not decision-grade: it claims an “execution vacuum” while incumbents clearly market execution features, and it lacks the feature matrix + evidence + economics needed to pick a wedge confidently.  ￼

If you want, I can turn this into a v2 research prompt that forces (1) the feature checklist, (2) review-quote tagging, and (3) unit-econ math—so the research output naturally produces a wedge decision instead of assuming one.


You’ve got a solid critique here. I agree with the core diagnosis (execution wedge is right) and I’m going to refine the plan around the big risk you called out:

Don’t spend your first 60 days rebuilding the incumbent moat (mapping). Ship the action layer first, powered by “bring-your-own-overlaps,” then earn the right to replace the data engine.

Below is the refined plan with concrete changes to strategy, architecture, MVP scope, and pricing.

⸻

1) Strategic refinement: “Execution OS” stays, but MVP becomes a Sidecar

What changes (and why)

Original plan flaw: it tries to validate “execution” while spending the first half of the build on “mapping.” That’s backwards.

Refined MVP approach (Sidecar):
	•	Ingest overlaps via CSV export (from Crossbeam/Reveal or any source) + optional lightweight CRM pull
	•	Deliver workflow addiction first: intros, tasks, Slack approvals, outcomes, attribution notes
	•	Build native mapping only after you’ve proven:
	1.	users act on overlaps, and
	2.	they come back weekly because of the workflow

This is the fastest path to proving the “execution gap” without competing head-on with Crossbeam’s network effects.

Why it’s viable in-market

Crossbeam already positions Connector as “turn insights into action” and prices Connector at $4,800/year for 1 seat, with additional seats $1,800/user/year.  ￼
So: execution exists, but the wedge is adoption + speed + rep-native loop closure, not “we also have Slack.”

⸻

2) Product wedge sharpened: “Close-the-loop co-sell” (rep-native, outcome-captured)

Your unique claim must be one level deeper than “Slack alerts.”

Killer feature statement

“One-click partner intro requests that close the loop: request → approve → book → log → attribute → write back.”

Incumbents help you find overlaps; many teams still fail at:
	•	intro request throughput
	•	response SLAs
	•	rep adoption
	•	outcome capture
	•	defensible attribution

That’s where you win.

⸻

3) MVP scope (Week-2 value): “Bring your overlaps; we make them actionable”

MVP capabilities (tight)
	1.	Overlap import + normalization
	2.	Shared list with workflow states
	3.	Intro request + approval loop
	4.	Slack interactive actions
	5.	Outcome capture + basic attribution notes
	6.	Webhook/event stream (internal + external)

What we explicitly DO NOT do in MVP
	•	Full CRM ingestion and bi-directional sync
	•	Full identity resolution across messy CRMs
	•	Multi-partner mapping at massive scale
	•	PRM portal features
	•	Complex probabilistic matching

⸻

4) Feature-by-feature MVP spec (and the “4 required anchors”)

Per your rule: every MVP feature must include (a) DB tables, (b) UI screens, (c) integration points, (d) KPI moved.

MVP Feature 1 — Overlap Import (CSV)
	•	DB: partner_connection, population, population_record, overlap, overlap_summary, audit_event
	•	UI: “Import overlaps” wizard; “Imports history” page; “Overlap list” page
	•	Integrations: none required (CSV); optional “Crossbeam export template”
	•	KPI: Time-to-first-value (TTFV) < 30 minutes; Imported overlaps count; % overlaps triaged

Acceptance criteria
	•	Upload CSV → validates → creates overlaps → user sees list within 60 seconds for <50k rows
	•	Idempotent re-import by source_run_id

⸻

MVP Feature 2 — Shared List + Ownership + States
	•	DB: shared_list, shared_list_item, assignment, comment, audit_event
	•	UI: Shared list page (filters + status); Overlap detail drawer (notes, owner, activity)
	•	Integrations: none required
	•	KPI: % overlaps assigned; median time-to-first-touch

States (minimal)
	•	new → requested_intro → intro_accepted → meeting_booked → opportunity_created → closed_won/lost

⸻

MVP Feature 3 — Intro Request Workflow
	•	DB: intro_request, intro_participant, action_outcome, audit_event
	•	UI: “Request intro” modal; “My requests” queue; Overlap detail timeline
	•	Integrations: Slack (send request + approve/deny); Email optional later
	•	KPI: intro request volume; acceptance rate; SLA to response

Acceptance criteria
	•	Request can be approved/denied from Slack Block Kit button
	•	Every state change writes an immutable audit event

⸻

MVP Feature 4 — Slack App (Interactive)
	•	DB: slack_installation, notification_delivery, audit_event
	•	UI: Slack settings page; Notification preview; Delivery logs
	•	Integrations: Slack OAuth + interactive callbacks
	•	KPI: % intros handled in Slack; response time; notification CTR

⸻

MVP Feature 5 — Outcome Capture + “Light Attribution”
	•	DB: outcome, touch, attribution_note, audit_event
	•	UI: “Log outcome” quick form; basic “Impact” dashboard
	•	Integrations: none required for MVP (writeback later)
	•	KPI: % requests with outcomes; influenced pipeline estimate (manual)

Important: Don’t pretend this is “full attribution.” It’s “audit-friendly outcome logging.”

⸻

5) Architecture fixes (Django + HAT), incorporating the critique

5.1 RLS: keep it, but design it like you mean it

The critique is right: RLS can add overhead, especially on joins. Real-world impact varies by policy shape and query patterns; you should assume there is a tax and design around it.  ￼

Refined approach
	•	Use tenant_id everywhere, index it everywhere, and include it in composite indexes for major queries.
	•	Keep RLS policies simple (no subqueries in policies).
	•	Still add explicit WHERE tenant_id = … filters in app queries to help the planner prune early (even with RLS).

Rule: RLS is safety net + compliance story. Query performance still needs explicit tenant predicates + good indexes.

⸻

5.2 Overlap engine: no materialized views for MVP

The critique is right: Postgres materialized views are full-refresh by default; incremental requires extensions like pg_ivm or custom maintenance.  ￼

Refined choice
	•	Use tables maintained by jobs:
	•	overlap (row-level)
	•	overlap_summary (aggregates)
	•	Recompute summary incrementally per import/job.

This is simpler, more controllable, and easier to scale.

⸻

5.3 Matching: MVP is deterministic only

MVP matching rules
	•	Normalize and match on domain exact only
	•	Everything else is “unmatched” unless user manually links it

Why: false positives destroy trust.

V1 (later): add fuzzy matching with pg_trgm, but only with blocking + review queue.

⸻

6) Data engine roadmap (so you eventually replace the CSV)

Phase 0 (MVP): BYO overlaps
	•	CSV import (Crossbeam/Reveal export)
	•	Optional: “manual add overlap” + “upload partner list”

Phase 1 (V1): Internal single-player value (no partner required)

This is your cold-start killer:
	•	connect HubSpot + Salesforce (or 2 sources) and detect internal misalignment
Examples:
	•	“Account is customer in CRM but still in marketing nurture”
	•	“Duplicate accounts across regions”
	•	“Open opp + partner product installed (from internal data)”

This wedge doesn’t require partners to onboard.

Phase 2 (V2): Native mapping
	•	Salesforce Bulk API 2.0 for initial loads is the correct path (don’t REST-page 100k accounts).
	•	Incremental sync via SystemModstamp + deleted records handling.
	•	Entity resolution:
	•	blocking (prefix/state/industry buckets)
	•	trigram similarity only within blocks
	•	review queue + “never match again” UI

⸻

7) Pricing correction (anchored to market reality)

Crossbeam’s Connector pricing is effectively $400/mo base for 1 seat (annualized) plus seats at $150/mo annualized.  ￼

If you show up at $1,000/mo as a new entrant, you’ll fight uphill.

Recommended packaging (starter-friendly + expansion)

Starter (Sidecar): $199/mo + $49/active seller seat
	•	Includes CSV imports, shared lists, Slack actions, outcome logging
	•	Limits: # active lists, # monthly intro requests

Pro: $499/mo + $79/seat
	•	Adds automation rules, webhooks, SLA dashboards, templates

Usage add-on (optional but powerful):
	•	$X per accepted intro above included quota
This aligns with outcomes without being fully “commission-based.”

⸻

8) Execution plan (0–30 / 31–60 / 61–90) revised

0–30 days: Ship the Sidecar MVP (prove execution)
	•	Overlap import + shared lists
	•	Intro workflow + Slack interactive approvals
	•	Outcome capture + basic dashboard
	•	Audit logs + event model

Success definition: 3 pilots, each generates 20+ intro requests in 30 days.

31–60 days: Make it “sticky” (automation + speed)
	•	Rules engine (simple JSON conditions → actions)
	•	Notification tuning + SLA metrics
	•	Templates + saved plays
	•	Better list segmentation + assignment routing

61–90 days: Add the single-player wedge + first connector
	•	Internal data hygiene module (1 connector to start, likely HubSpot)
	•	Basic writeback (create CRM note/task only—avoid complex objects)
	•	Matching V1 (domain exact + manual links; no fuzzy by default)

⸻

9) Updated risk register (top pilot killers + prevention)
	1.	No partner participation → MVP doesn’t require partner sign-up (CSV import works)
	2.	Reps ignore it → Slack-first actions + outcome capture is rep-native
	3.	Bad matches ruin trust → deterministic matching only + manual link/reject
	4.	CRM integration drags timeline → delayed to V1/V2; MVP is connector-light
	5.	Security review stalls → audit events + least-privilege + clear data boundaries from day 1

⸻

The refined “north star”

Build the co-sell operating layer that teams use daily, regardless of where overlaps come from.
	•	MVP: “Bring overlaps → execute intros in Slack → log outcomes”
	•	V1: “Get value without partners (internal hygiene)”
	•	V2: “Replace overlap source with your own mapping engine”

If you want, I’ll convert this into a single cohesive PRD + architecture spec (MVP Sidecar version) with: Django app structure, models, endpoints, Slack payload schemas, and the event/playbook DSL—so you can hand it to a builder and start immediately.



Absolutely — here’s a practical Django skeleton + core models you can paste into Cursor and start running immediately.

I’m optimizing for:
	•	Sidecar MVP (CSV overlaps → Slack intro approvals → outcomes → audit log)
	•	HAT stack-ready (server templates + HTMX)
	•	Multi-tenant by tenant_id (no RLS yet, but structured for it)

⸻

0) Quick start commands

# create venv + install deps
python -m venv .venv
source .venv/bin/activate
pip install "Django>=5.0,<6.0" psycopg[binary] django-environ django-htmx celery redis

# start project
django-admin startproject cosell .
mkdir -p apps/integrations/slack
touch apps/__init__.py apps/integrations/__init__.py apps/integrations/slack/__init__.py


⸻

1) Suggested repo structure

.
├── manage.py
├── cosell/
│   ├── __init__.py
│   ├── settings.py
│   ├── urls.py
│   ├── celery.py
│   └── wsgi.py
└── apps/
    ├── core/
    ├── accounts/
    ├── partners/
    ├── overlaps/
    ├── intros/
    ├── audit/
    └── integrations/
        └── slack/

Create apps:

python manage.py startapp core apps/core
python manage.py startapp accounts apps/accounts
python manage.py startapp partners apps/partners
python manage.py startapp overlaps apps/overlaps
python manage.py startapp intros apps/intros
python manage.py startapp audit apps/audit
python manage.py startapp slack apps/integrations/slack


⸻

2) Settings + Celery wiring

cosell/celery.py

import os
from celery import Celery

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "cosell.settings")

app = Celery("cosell")
app.config_from_object("django.conf:settings", namespace="CELERY")
app.autodiscover_tasks()

cosell/__init__.py

from .celery import app as celery_app

__all__ = ("celery_app",)

cosell/settings.py (minimal edits)

from pathlib import Path
import environ

BASE_DIR = Path(__file__).resolve().parent.parent
env = environ.Env(DEBUG=(bool, False))
environ.Env.read_env(BASE_DIR / ".env")

SECRET_KEY = env("SECRET_KEY", default="dev-secret-key")
DEBUG = env("DEBUG", default=True)
ALLOWED_HOSTS = env.list("ALLOWED_HOSTS", default=["*"])

INSTALLED_APPS = [
    "django.contrib.admin",
    "django.contrib.auth",
    "django.contrib.contenttypes",
    "django.contrib.sessions",
    "django.contrib.messages",
    "django.contrib.staticfiles",

    # optional but handy for HTMX request detection
    "django_htmx",

    # your apps
    "apps.core",
    "apps.accounts",
    "apps.partners",
    "apps.overlaps",
    "apps.intros",
    "apps.audit",
    "apps.integrations.slack",
]

MIDDLEWARE = [
    "django.middleware.security.SecurityMiddleware",
    "django.contrib.sessions.middleware.SessionMiddleware",
    "django.middleware.common.CommonMiddleware",
    "django.middleware.csrf.CsrfViewMiddleware",
    "django.contrib.auth.middleware.AuthenticationMiddleware",
    "django.contrib.messages.middleware.MessageMiddleware",
    "django.middleware.clickjacking.XFrameOptionsMiddleware",

    "django_htmx.middleware.HtmxMiddleware",

    # tenant selection for requests (very light version)
    "apps.core.middleware.TenantMiddleware",
]

ROOT_URLCONF = "cosell.urls"

TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "DIRS": [BASE_DIR / "templates"],
        "APP_DIRS": True,
        "OPTIONS": {
            "context_processors": [
                "django.template.context_processors.request",
                "django.contrib.auth.context_processors.auth",
                "django.contrib.messages.context_processors.messages",
                "apps.core.context_processors.current_tenant",
            ],
        },
    }
]

WSGI_APPLICATION = "cosell.wsgi.application"

DATABASES = {
    "default": env.db(default=f"postgres://postgres:postgres@localhost:5432/cosell")
}

AUTH_PASSWORD_VALIDATORS = [
    {"NAME": "django.contrib.auth.password_validation.UserAttributeSimilarityValidator"},
    {"NAME": "django.contrib.auth.password_validation.MinimumLengthValidator"},
    {"NAME": "django.contrib.auth.password_validation.CommonPasswordValidator"},
    {"NAME": "django.contrib.auth.password_validation.NumericPasswordValidator"},
]

LANGUAGE_CODE = "en-us"
TIME_ZONE = "America/Chicago"
USE_I18N = True
USE_TZ = True

STATIC_URL = "static/"
DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"

# Celery
CELERY_BROKER_URL = env("CELERY_BROKER_URL", default="redis://localhost:6379/0")
CELERY_RESULT_BACKEND = env("CELERY_RESULT_BACKEND", default="redis://localhost:6379/1")
CELERY_TASK_ALWAYS_EAGER = env.bool("CELERY_TASK_ALWAYS_EAGER", default=False)

# Slack
SLACK_CLIENT_ID = env("SLACK_CLIENT_ID", default="")
SLACK_CLIENT_SECRET = env("SLACK_CLIENT_SECRET", default="")
SLACK_SIGNING_SECRET = env("SLACK_SIGNING_SECRET", default="")

cosell/urls.py

from django.contrib import admin
from django.urls import path

urlpatterns = [
    path("admin/", admin.site.urls),
]


⸻

3) Core multi-tenant scaffolding

apps/core/models.py

from django.db import models
from django.utils import timezone


class TimeStampedModel(models.Model):
    created_at = models.DateTimeField(default=timezone.now, editable=False)
    updated_at = models.DateTimeField(auto_now=True)

    class Meta:
        abstract = True


class Tenant(TimeStampedModel):
    name = models.CharField(max_length=200)
    slug = models.SlugField(max_length=80, unique=True)  # used for subdomain or URL prefix
    is_active = models.BooleanField(default=True)

    def __str__(self) -> str:
        return self.name

apps/core/middleware.py

from __future__ import annotations
from typing import Optional
from django.http import HttpRequest
from apps.core.models import Tenant

TENANT_ATTR = "tenant"


def _tenant_from_request(request: HttpRequest) -> Optional[Tenant]:
    """
    MVP tenant selection:
    - Prefer session["tenant_slug"] if set
    - Else, if only one tenant exists, use it (dev-friendly)
    - Else, leave None (you can enforce later)
    """
    slug = request.session.get("tenant_slug")
    if slug:
        return Tenant.objects.filter(slug=slug, is_active=True).first()

    qs = Tenant.objects.filter(is_active=True).order_by("id")
    if qs.count() == 1:
        return qs.first()
    return None


class TenantMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response

    def __call__(self, request: HttpRequest):
        request.tenant = _tenant_from_request(request)  # type: ignore[attr-defined]
        return self.get_response(request)

apps/core/context_processors.py

def current_tenant(request):
    return {"current_tenant": getattr(request, "tenant", None)}


⸻

4) Accounts (tenant membership)

apps/accounts/models.py

from django.conf import settings
from django.db import models
from apps.core.models import Tenant, TimeStampedModel


class MembershipRole(models.TextChoices):
    OWNER = "owner", "Owner"
    ADMIN = "admin", "Admin"
    MEMBER = "member", "Member"


class Membership(TimeStampedModel):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="memberships")
    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE, related_name="memberships")
    role = models.CharField(max_length=20, choices=MembershipRole.choices, default=MembershipRole.MEMBER)
    is_active = models.BooleanField(default=True)

    class Meta:
        unique_together = [("tenant", "user")]
        indexes = [
            models.Index(fields=["tenant", "user"]),
            models.Index(fields=["tenant", "role"]),
        ]

    def __str__(self) -> str:
        return f"{self.user_id} @ {self.tenant_id} ({self.role})"


⸻

5) Partners + partner connections

apps/partners/models.py

from django.db import models
from apps.core.models import TimeStampedModel, Tenant


class Partner(TimeStampedModel):
    """
    A partner org. In MVP you can represent partners locally (no need for them to "sign up").
    """
    name = models.CharField(max_length=255)
    domain = models.CharField(max_length=255, blank=True, default="")  # optional

    def __str__(self) -> str:
        return self.name


class PartnerConnectionStatus(models.TextChoices):
    ACTIVE = "active", "Active"
    PAUSED = "paused", "Paused"
    ARCHIVED = "archived", "Archived"


class PartnerConnection(TimeStampedModel):
    """
    A tenant's relationship to a partner.
    """
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="partner_connections")
    partner = models.ForeignKey(Partner, on_delete=models.CASCADE, related_name="connections")
    status = models.CharField(max_length=20, choices=PartnerConnectionStatus.choices, default=PartnerConnectionStatus.ACTIVE)

    # lightweight permissions toggles for MVP (expand later)
    allow_intro_requests = models.BooleanField(default=True)

    class Meta:
        unique_together = [("tenant", "partner")]
        indexes = [
            models.Index(fields=["tenant", "status"]),
            models.Index(fields=["tenant", "partner"]),
        ]

    def __str__(self) -> str:
        return f"{self.tenant} ↔ {self.partner}"


⸻

6) Overlaps + CSV import run

apps/overlaps/utils.py

from urllib.parse import urlparse

def normalize_domain(value: str) -> str:
    v = (value or "").strip().lower()
    if not v:
        return ""
    # allow full URLs
    if "://" in v:
        v = urlparse(v).netloc
    v = v.replace("www.", "")
    # strip path if user pasted domain/path
    v = v.split("/")[0]
    return v

apps/overlaps/models.py

from django.conf import settings
from django.db import models
from django.utils import timezone
from apps.core.models import Tenant, TimeStampedModel
from apps.partners.models import PartnerConnection
from .utils import normalize_domain


class ImportStatus(models.TextChoices):
    PENDING = "pending", "Pending"
    PROCESSING = "processing", "Processing"
    COMPLETED = "completed", "Completed"
    FAILED = "failed", "Failed"


class OverlapImportRun(TimeStampedModel):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="overlap_imports")
    partner_connection = models.ForeignKey(PartnerConnection, on_delete=models.CASCADE, related_name="imports")

    source = models.CharField(max_length=50, default="csv")  # csv, crossbeam, reveal, etc.
    source_run_id = models.CharField(max_length=128, blank=True, default="")  # idempotency helper
    status = models.CharField(max_length=20, choices=ImportStatus.choices, default=ImportStatus.PENDING)

    started_at = models.DateTimeField(null=True, blank=True)
    finished_at = models.DateTimeField(null=True, blank=True)
    error = models.TextField(blank=True, default="")

    row_count = models.PositiveIntegerField(default=0)
    created_overlap_count = models.PositiveIntegerField(default=0)
    updated_overlap_count = models.PositiveIntegerField(default=0)

    def mark_processing(self):
        self.status = ImportStatus.PROCESSING
        self.started_at = timezone.now()

    def mark_completed(self):
        self.status = ImportStatus.COMPLETED
        self.finished_at = timezone.now()

    def mark_failed(self, error: str):
        self.status = ImportStatus.FAILED
        self.finished_at = timezone.now()
        self.error = error


class OverlapStatus(models.TextChoices):
    NEW = "new", "New"
    REQUESTED_INTRO = "requested_intro", "Intro Requested"
    APPROVED = "approved", "Approved"
    DENIED = "denied", "Denied"
    IN_PROGRESS = "in_progress", "In Progress"
    OUTCOME_LOGGED = "outcome_logged", "Outcome Logged"


class Overlap(TimeStampedModel):
    """
    Represents an overlapped account between tenant and partner.
    In MVP, this is imported (bring-your-own overlaps).
    """
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="overlaps")
    partner_connection = models.ForeignKey(PartnerConnection, on_delete=models.CASCADE, related_name="overlaps")

    # normalized company identifiers
    account_name = models.CharField(max_length=255)
    account_domain = models.CharField(max_length=255, blank=True, default="")
    account_domain_norm = models.CharField(max_length=255, blank=True, default="")

    # helpful metadata
    segment = models.CharField(max_length=100, blank=True, default="")
    internal_owner_name = models.CharField(max_length=200, blank=True, default="")
    partner_owner_name = models.CharField(max_length=200, blank=True, default="")

    status = models.CharField(max_length=30, choices=OverlapStatus.choices, default=OverlapStatus.NEW)

    # idempotency + freshness
    source_run = models.ForeignKey(OverlapImportRun, on_delete=models.SET_NULL, null=True, blank=True, related_name="overlap_rows")
    source_row_key = models.CharField(max_length=200, blank=True, default="")  # stable unique key per row
    last_seen_at = models.DateTimeField(default=timezone.now)

    created_by = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True, blank=True, related_name="created_overlaps")

    class Meta:
        indexes = [
            models.Index(fields=["tenant", "partner_connection", "status"]),
            models.Index(fields=["tenant", "account_domain_norm"]),
            models.Index(fields=["tenant", "partner_connection", "account_domain_norm"]),
            models.Index(fields=["tenant", "last_seen_at"]),
        ]

    def save(self, *args, **kwargs):
        self.account_domain_norm = normalize_domain(self.account_domain)
        super().save(*args, **kwargs)

    def __str__(self) -> str:
        return f"{self.account_name} ({self.account_domain_norm})"


⸻

7) Intros + outcomes

apps/intros/models.py

from django.conf import settings
from django.db import models
from django.utils import timezone
from apps.core.models import Tenant, TimeStampedModel
from apps.overlaps.models import Overlap


class IntroStatus(models.TextChoices):
    DRAFT = "draft", "Draft"
    SENT = "sent", "Sent"
    APPROVED = "approved", "Approved"
    DENIED = "denied", "Denied"
    EXPIRED = "expired", "Expired"
    CANCELLED = "cancelled", "Cancelled"
    COMPLETED = "completed", "Completed"


class IntroRequest(TimeStampedModel):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="intro_requests")
    overlap = models.ForeignKey(Overlap, on_delete=models.CASCADE, related_name="intro_requests")

    requested_by = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True, related_name="intro_requests_made")

    message = models.TextField(blank=True, default="")
    status = models.CharField(max_length=20, choices=IntroStatus.choices, default=IntroStatus.DRAFT)

    sent_at = models.DateTimeField(null=True, blank=True)
    decided_at = models.DateTimeField(null=True, blank=True)
    decision_by = models.CharField(max_length=200, blank=True, default="")  # partner user display name/email if known
    decision_reason = models.TextField(blank=True, default="")

    # Slack message linkage for interactive approvals
    slack_channel_id = models.CharField(max_length=64, blank=True, default="")
    slack_message_ts = models.CharField(max_length=64, blank=True, default="")

    class Meta:
        indexes = [
            models.Index(fields=["tenant", "status", "created_at"]),
            models.Index(fields=["tenant", "overlap"]),
        ]

    def mark_sent(self):
        self.status = IntroStatus.SENT
        self.sent_at = timezone.now()

    def mark_approved(self, decision_by: str = ""):
        self.status = IntroStatus.APPROVED
        self.decided_at = timezone.now()
        self.decision_by = decision_by

    def mark_denied(self, decision_by: str = "", reason: str = ""):
        self.status = IntroStatus.DENIED
        self.decided_at = timezone.now()
        self.decision_by = decision_by
        self.decision_reason = reason


class OutcomeType(models.TextChoices):
    MEETING_BOOKED = "meeting_booked", "Meeting Booked"
    OPP_CREATED = "opp_created", "Opportunity Created"
    CLOSED_WON = "closed_won", "Closed Won"
    CLOSED_LOST = "closed_lost", "Closed Lost"
    NO_RESPONSE = "no_response", "No Response"
    NOT_A_FIT = "not_a_fit", "Not a Fit"


class Outcome(TimeStampedModel):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="outcomes")
    overlap = models.ForeignKey(Overlap, on_delete=models.CASCADE, related_name="outcomes")
    intro_request = models.ForeignKey(IntroRequest, on_delete=models.SET_NULL, null=True, blank=True, related_name="outcomes")

    logged_by = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True, related_name="outcomes_logged")
    outcome_type = models.CharField(max_length=30, choices=OutcomeType.choices)
    notes = models.TextField(blank=True, default="")

    # light ROI fields (optional)
    estimated_pipeline = models.DecimalField(max_digits=12, decimal_places=2, null=True, blank=True)
    estimated_revenue = models.DecimalField(max_digits=12, decimal_places=2, null=True, blank=True)

    class Meta:
        indexes = [
            models.Index(fields=["tenant", "outcome_type", "created_at"]),
            models.Index(fields=["tenant", "overlap"]),
        ]


⸻

8) Audit event log (immutable-ish)

apps/audit/models.py

from django.conf import settings
from django.db import models
from django.utils import timezone
from apps.core.models import Tenant


class AuditEvent(models.Model):
    """
    Append-only event log. Avoid updating rows after creation.
    """
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="audit_events")
    actor = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True, blank=True)

    event_type = models.CharField(max_length=100)  # e.g., intro_request.approved
    object_type = models.CharField(max_length=100, blank=True, default="")  # e.g., IntroRequest
    object_id = models.CharField(max_length=64, blank=True, default="")     # store as string for flexibility
    payload = models.JSONField(default=dict, blank=True)

    occurred_at = models.DateTimeField(default=timezone.now, db_index=True)

    class Meta:
        indexes = [
            models.Index(fields=["tenant", "event_type", "occurred_at"]),
            models.Index(fields=["tenant", "object_type", "object_id"]),
        ]

    def __str__(self) -> str:
        return f"{self.tenant_id}:{self.event_type}@{self.occurred_at.isoformat()}"


⸻

9) Slack installation + delivery logs (core tables only)

apps/integrations/slack/models.py

from django.db import models
from django.utils import timezone
from apps.core.models import Tenant, TimeStampedModel


class SlackInstallation(TimeStampedModel):
    tenant = models.OneToOneField(Tenant, on_delete=models.CASCADE, related_name="slack_installation")

    team_id = models.CharField(max_length=64)
    team_name = models.CharField(max_length=200, blank=True, default="")

    bot_user_id = models.CharField(max_length=64, blank=True, default="")
    bot_token = models.TextField(blank=True, default="")  # store encrypted in prod

    installed_by = models.CharField(max_length=200, blank=True, default="")
    installed_at = models.DateTimeField(default=timezone.now)

    def __str__(self) -> str:
        return f"Slack({self.team_id}) for tenant {self.tenant_id}"


class SlackDeliveryStatus(models.TextChoices):
    SENT = "sent", "Sent"
    FAILED = "failed", "Failed"


class SlackMessageDelivery(TimeStampedModel):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="slack_deliveries")

    channel_id = models.CharField(max_length=64)
    message_ts = models.CharField(max_length=64, blank=True, default="")
    status = models.CharField(max_length=10, choices=SlackDeliveryStatus.choices, default=SlackDeliveryStatus.SENT)

    request_idempotency_key = models.CharField(max_length=128, blank=True, default="")
    error = models.TextField(blank=True, default="")

    class Meta:
        indexes = [
            models.Index(fields=["tenant", "status", "created_at"]),
            models.Index(fields=["tenant", "request_idempotency_key"]),
        ]


⸻

10) Register apps in admin (minimal)

Example: apps/overlaps/admin.py

from django.contrib import admin
from .models import Overlap, OverlapImportRun

@admin.register(OverlapImportRun)
class OverlapImportRunAdmin(admin.ModelAdmin):
    list_display = ("id", "tenant", "partner_connection", "source", "status", "row_count", "created_at")
    list_filter = ("status", "source")
    search_fields = ("source_run_id",)

@admin.register(Overlap)
class OverlapAdmin(admin.ModelAdmin):
    list_display = ("id", "tenant", "partner_connection", "account_name", "account_domain_norm", "status", "last_seen_at")
    list_filter = ("status",)
    search_fields = ("account_name", "account_domain_norm")

(Do similar for IntroRequest, Outcome, AuditEvent, SlackInstallation.)

⸻

11) Migrate + run

python manage.py makemigrations
python manage.py migrate
python manage.py createsuperuser
python manage.py runserver


⸻

Next step (recommended thin slice)

If you say “generate CSV import service + HTMX overlap inbox + intro request + Slack approve/deny endpoint”, I’ll output a complete next bundle with:
	•	import service (streaming CSV),
	•	overlap list + detail drawer templates,
	•	intro request create view,
	•	Slack signature verification + interactive payload handler,
	•	audit event helpers,
	•	and Celery tasks for reminders.